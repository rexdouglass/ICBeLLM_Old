{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity Extraction and Coerefernce resolution\n",
    "variable_name = \"yhat_references\"\n",
    "max_new_tokens=250\n",
    "temperature=0.7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ/discussions/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece\n",
    "#!pip install tokenizers -U\n",
    "#!pip install transformers[sentencepiece] -U\n",
    "#!pip install accelerate -U\n",
    "#!pip install optimum\n",
    "\n",
    "#https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ/discussions/5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/skynet3/8tb_a/rwd_github_private/ICBeLLM'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#https://stackoverflow.com/questions/73747731/runtimeerror-cuda-out-of-memory-how-setting-max-split-size-mb\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\" #experiment with this and the value, see if it prevents oom\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "cwd\n",
    "os.chdir('/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/')\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" #auto gets confused for some reason and puts it on the 2080\n",
    "import torch\n",
    "torch.cuda.device_count()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(device=None)\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skynet3/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "\n",
    "# If you've already downloaded the model, reference its location here:\n",
    "#quantized_model_dir = \"/path/to/TheBloke_WizardLM-Uncensored-Falcon-40B-3bit-GPTQ\"\n",
    "# Or to download it from the hub and store it in the Hugging Face cache directory:\n",
    "\n",
    "#quantized_model_dir = \"./data_temp/Wizard-Vicuna-13B-Uncensored-GPTQ/\" #TheBloke_WizardLM-Uncensored-Falcon-40B-3bit-GPTQ\n",
    "#quantized_model_dir = \"TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ\"\n",
    "quantized_model_dir = \"TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ\"\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The safetensors archive passed at /home/skynet3/.cache/huggingface/hub/models--TheBloke--Wizard-Vicuna-30B-Uncensored-GPTQ/snapshots/7aeb8db6d49732afa76dc7ce60c5c040dbbcf3ea/Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    }
   ],
   "source": [
    "#FileNotFoundError: Could not find model in TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ\n",
    "#https://huggingface.co/TheBloke/wizard-vicuna-13B-GPTQ/discussions/7\n",
    "#quantized_model_dir = \"TheBloke/stable-vicuna-13B-GPTQ\"\n",
    "#model_basename = \"wizard-vicuna-13B-GPTQ-4bit.compat.no-act-order\" #YOU HAVE TO DO THIS\n",
    "\n",
    "#model_basename = \"Wizard-vicuna-30B-Uncensored-GPTQ-4bit.act-order\"\n",
    "# path to directory containing local model\n",
    "#https://huggingface.co/TheBloke/starcoderplus-GPTQ/discussions/3\n",
    "quantized_model_dir = \"TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ\" #\"/workspace/models/TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ\"\n",
    "model_basename = \"Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order\" #.safetensors\n",
    "use_triton = False \n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, \n",
    "                                           device=\"cuda:0\", \n",
    "                                           use_triton=False,\n",
    "                                           use_safetensors=True , \n",
    "                                           model_basename=model_basename,\n",
    "                                           quantize_config=None,\n",
    "                                           #torch_dtype=torch.bfloat16, \n",
    "                                           trust_remote_code=True\n",
    "                                           ) #\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hf_device_map #yup, it offloaded the last layers to the CPU. That's why it's so slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Output ***\n",
      "<s> ### Human: Ask Something here\n",
      "### Assistant: Sure, I can help you with anything related to the topic of this book. What would you like to know?</s>\n",
      "**************\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Ask Something here\"\n",
    "prompt_model = f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "input_ids = tokenizer(prompt_model, return_tensors=\"pt\").input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "\n",
    "print(\"*** Output ***\")\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(\"**************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok it continues to be about 4 characters per token\n",
    "# print(input_ids.shape) #So this prompt and a little bit of story is about 939 tokens, got another 1k to play with\n",
    "#print(len(prompt))\n",
    "#5046/1287\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_base = \"\"\"You are a natural language processing pipeline. You read a paragraph of text and then perform a task on final sentence. The task is to create a semicolon separated list of entities mentioned in final sentence. Perform coreference resolution and map ambiguous references to named entities mentioned earlier in the paragraph. Think step by step. Explain your reasoning and then give a final answer.\n",
    "\n",
    "### Paragraph:\n",
    "The Communist regime, which had attained power in Russia on 7 November 1917, opted to withdraw from World War I through a separate peace with Germany--the Treaty of Brest-Litovsk on 3 March 1918.\n",
    "\n",
    "### Final Sentence:\n",
    "The Western Allies were anxious to maintain an eastern front against Germany and wished to prevent the Germans from seizing large stocks of arms in Vladivostok.\n",
    "\n",
    "### Thought:\n",
    "The final sentence to process is \"The Western Allies were anxious to maintain an eastern front against Germany and wished to prevent the Germans from seizing large stocks of arms in Vladivostok.\"\n",
    "The references in that final sentence are \"Western Allies\", \"Germany\", \"eastern front\", and \"Vladivostok\".\n",
    "\"Western Allies\" refers to a coalition of countries who are not explicitly listed yet in the paragraph and so include \"Western Allies\" in the list.\n",
    "\"Germany\" directly refers to a country and so include \"Germany\" in the list.\n",
    "\"eastern front\" indirectly refers to the country \"Russia\" so include \"Russia\" in the list.\n",
    "\"Vladivostok\" directly refers to a city so include \"Vladivostok\" in the list.\n",
    "\n",
    "### Final Answer:\n",
    "Western Allies; Germany; Russia; Vladivostok\n",
    "\n",
    "### Paragraph:\n",
    "Three of the Austro-Hungarian Empire's successor states--Czechoslovakia, Yugoslavia, and Hungary--experienced a two-phase crisis from 27 March until 15 November 1921. Karl IV, the last Hapsburg Emperor of Austria-Hungary, abdicated in November 1918 and was exiled to Switzerland; but he did not abdicate his right to the throne. Early in 1921 the Conference of Ambassadors, acting for the Supreme Council (Big Ten) at the Paris Peace Conference, issued a decree forbidding the restoration of the Hapsburg family to the vacant Hungarian throne. Moreover, several European governments issued statements indicating what recourse they might have if a Hapsburg reascended the throne in Budapest. On 27 March 1921 Karl unexpectedly returned to his homeland and called upon the Regent of Hungary, Admiral Horthy, to transfer his powers to the rightful ruler. This act triggered a crisis for Czechoslovakia and Yugoslavia.\n",
    "\n",
    "### Final Sentence:\n",
    "The latter responded two days later by declaring that it would regard any attempt to restore Karl as a casus belli.\n",
    "\n",
    "### Thought:\n",
    "The final sentence to process is \"The latter responded two days later by declaring that it would regard any attempt to restore Karl as a casus belli.\"\n",
    "The references in that final sentence are \"The latter\", \"attempt to restore Karl\", and \"Karl\".\n",
    "\"The latter' indirectly refers to the entity \"Yugoslavia\" from the previous sentence so include \"Yugoslavia\" in the list.\n",
    "\"any attempt to restore Karl\" indirectly refers to an action by the country \"Hungary\" so include Hungary in the list.\n",
    "\"Karl\" refers to the entity \"Karl IV\" introduced earlier in the paragraph so include \"Karl IV\" in the list.\n",
    "\n",
    "### Final Answer:\n",
    "Yugoslavia; Hungary; Karl IV\n",
    "\n",
    "### Paragraph:\n",
    "Cuba and the United States were the adversaries in a crisis from 15 to 24 April 1961. A series of incursions into Caribbean and Central American states, in which Cuba was perceived to be involved, took place in 1959-60 In addition, the U.S. had become increasingly concerned about the strong ties that had developed between Cuba and the USSR. Relations between the U.S. and Cuba were strained; and the latter appealed to the Security Council at the end of 1960 charging the U.S. with plans to mount an invasion of Cuba using Cuban exiles trained in Guatemala. Diplomatic relations between the two countries were broken on 3 January 1961. Cuba and the United States were the adversaries in a crisis from 15 to 24 April 1961. A series of incursions into Caribbean and Central American states, in which Cuba was perceived to be involved, took place in 1959-60 In addition, the U.S. had become increasingly concerned about the strong ties that had developed between Cuba and the USSR. Relations between the U.S. and Cuba were strained; and the latter appealed to the Security Council at the end of 1960 charging the U.S. with plans to mount an invasion of Cuba using Cuban exiles trained in Guatemala. Diplomatic relations between the two countries were broken on 3 January 1961.\n",
    "\n",
    "### Final Sentence:\n",
    "In early April 1961 President Kennedy authorized an invasion of Cuba by the exiles, reasoning that, in the event of a failure, the remaining force would be able to establish guerrilla bases in the Cuban mountains.\n",
    "\n",
    "### Thought: \n",
    "The final sentence to process is \"In early April 1961 President Kennedy authorized an invasion of Cuba by the exiles, reasoning that, in the event of a failure, the remaining force would be able to establish guerrilla bases in the Cuban mountains.\"\n",
    "The references in that final sentence are \"President Kennedy\", \"Cuba\", \"exiles\", \"guerrilla bases\", and \"Cuban mountains\".\n",
    "\"President Kennedy\" refers to the entity \"President John F. Kennedy\" and so include \"President John F. Kennedy\" in the list.\n",
    "\"Cuba\" directly refers to the country \"Cuba\" and so include \"Cuba\" in the list.\n",
    "\"exiles\" refers to the group \"Cuban exiles\" introduced earlier in the paragraph and so include \"Cuban exiles\" in the list.\n",
    "\"Cuban mountains\" directly refers to a geographic feature \"Cuban mountains\" so include \"Cuban exiles\" in the list.\n",
    "\n",
    "### Final Answer:\n",
    "President John F. Kennedy; Cuba; Cuban exiles; Cuban mountains\n",
    "\n",
    "### Paragraph:\"\"\"\n",
    "\n",
    "this_obs = \"\"\"\n",
    "India experienced a crisis over Hyderabad from 21 August to 18 September 1948. Hyderabad was the second largest princely state in the subcontinent, after Jammu and Kashmir. It is located in the geographic center of India and, in 1948, 80 percent of its population was Hindu while most of the political elite were Muslim. The nizam of Hyderabad did not favor accession to distant Pakistan.\n",
    "\n",
    "### Final Sentence:\n",
    "Nor did predominantly Hindu India attract him.\n",
    "\"\"\"\n",
    "prompt = prompt_base + this_obs\n",
    "prompt_model = f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "input_ids = tokenizer(prompt_model, return_tensors=\"pt\").input_ids.cuda()\n",
    "#print(input_ids.shape) #Ok if we pass too long a prompt it goes OOM. We have to catch that. #2048\n",
    "assert(input_ids.shape[1]+250 < 2048 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape[1]+250 < 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: You are a natural language processing pipeline. You read a paragraph of text and then perform a task on final sentence. The task is to create a semicolon separated list of entities mentioned in final sentence. Perform coreference resolution and map ambiguous references to named entities mentioned earlier in the paragraph. Think step by step. Explain your reasoning and then give a final answer.\n",
      "\n",
      "### Paragraph:\n",
      "The Communist regime, which had attained power in Russia on 7 November 1917, opted to withdraw from World War I through a separate peace with Germany--the Treaty of Brest-Litovsk on 3 March 1918.\n",
      "\n",
      "### Final Sentence:\n",
      "The Western Allies were anxious to maintain an eastern front against Germany and wished to prevent the Germans from seizing large stocks of arms in Vladivostok.\n",
      "\n",
      "### Thought:\n",
      "The final sentence to process is \"The Western Allies were anxious to maintain an eastern front against Germany and wished to prevent the Germans from seizing large stocks of arms in Vladivostok.\"\n",
      "The references in that final sentence are \"Western Allies\", \"Germany\", \"eastern front\", and \"Vladivostok\".\n",
      "\"Western Allies\" refers to a coalition of countries who are not explicitly listed yet in the paragraph and so include \"Western Allies\" in the list.\n",
      "\"Germany\" directly refers to a country and so include \"Germany\" in the list.\n",
      "\"eastern front\" indirectly refers to the country \"Russia\" so include \"Russia\" in the list.\n",
      "\"Vladivostok\" directly refers to a city so include \"Vladivostok\" in the list.\n",
      "\n",
      "### Final Answer:\n",
      "Western Allies; Germany; Russia; Vladivostok\n",
      "\n",
      "### Paragraph:\n",
      "Three of the Austro-Hungarian Empire's successor states--Czechoslovakia, Yugoslavia, and Hungary--experienced a two-phase crisis from 27 March until 15 November 1921. Karl IV, the last Hapsburg Emperor of Austria-Hungary, abdicated in November 1918 and was exiled to Switzerland; but he did not abdicate his right to the throne. Early in 1921 the Conference of Ambassadors, acting for the Supreme Council (Big Ten) at the Paris Peace Conference, issued a decree forbidding the restoration of the Hapsburg family to the vacant Hungarian throne. Moreover, several European governments issued statements indicating what recourse they might have if a Hapsburg reascended the throne in Budapest. On 27 March 1921 Karl unexpectedly returned to his homeland and called upon the Regent of Hungary, Admiral Horthy, to transfer his powers to the rightful ruler. This act triggered a crisis for Czechoslovakia and Yugoslavia.\n",
      "\n",
      "### Final Sentence:\n",
      "The latter responded two days later by declaring that it would regard any attempt to restore Karl as a casus belli.\n",
      "\n",
      "### Thought:\n",
      "The final sentence to process is \"The latter responded two days later by declaring that it would regard any attempt to restore Karl as a casus belli.\"\n",
      "The references in that final sentence are \"The latter\", \"attempt to restore Karl\", and \"Karl\".\n",
      "\"The latter' indirectly refers to the entity \"Yugoslavia\" from the previous sentence so include \"Yugoslavia\" in the list.\n",
      "\"any attempt to restore Karl\" indirectly refers to an action by the country \"Hungary\" so include Hungary in the list.\n",
      "\"Karl\" refers to the entity \"Karl IV\" introduced earlier in the paragraph so include \"Karl IV\" in the list.\n",
      "\n",
      "### Final Answer:\n",
      "Yugoslavia; Hungary; Karl IV\n",
      "\n",
      "### Paragraph:\n",
      "Cuba and the United States were the adversaries in a crisis from 15 to 24 April 1961. A series of incursions into Caribbean and Central American states, in which Cuba was perceived to be involved, took place in 1959-60 In addition, the U.S. had become increasingly concerned about the strong ties that had developed between Cuba and the USSR. Relations between the U.S. and Cuba were strained; and the latter appealed to the Security Council at the end of 1960 charging the U.S. with plans to mount an invasion of Cuba using Cuban exiles trained in Guatemala. Diplomatic relations between the two countries were broken on 3 January 1961. Cuba and the United States were the adversaries in a crisis from 15 to 24 April 1961. A series of incursions into Caribbean and Central American states, in which Cuba was perceived to be involved, took place in 1959-60 In addition, the U.S. had become increasingly concerned about the strong ties that had developed between Cuba and the USSR. Relations between the U.S. and Cuba were strained; and the latter appealed to the Security Council at the end of 1960 charging the U.S. with plans to mount an invasion of Cuba using Cuban exiles trained in Guatemala. Diplomatic relations between the two countries were broken on 3 January 1961.\n",
      "\n",
      "### Final Sentence:\n",
      "In early April 1961 President Kennedy authorized an invasion of Cuba by the exiles, reasoning that, in the event of a failure, the remaining force would be able to establish guerrilla bases in the Cuban mountains.\n",
      "\n",
      "### Thought: \n",
      "The final sentence to process is \"In early April 1961 President Kennedy authorized an invasion of Cuba by the exiles, reasoning that, in the event of a failure, the remaining force would be able to establish guerrilla bases in the Cuban mountains.\"\n",
      "The references in that final sentence are \"President Kennedy\", \"Cuba\", \"exiles\", \"guerrilla bases\", and \"Cuban mountains\".\n",
      "\"President Kennedy\" refers to the entity \"President John F. Kennedy\" and so include \"President John F. Kennedy\" in the list.\n",
      "\"Cuba\" directly refers to the country \"Cuba\" and so include \"Cuba\" in the list.\n",
      "\"exiles\" refers to the group \"Cuban exiles\" introduced earlier in the paragraph and so include \"Cuban exiles\" in the list.\n",
      "\"Cuban mountains\" directly refers to a geographic feature \"Cuban mountains\" so include \"Cuban exiles\" in the list.\n",
      "\n",
      "### Final Answer:\n",
      "President John F. Kennedy; Cuba; Cuban exiles; Cuban mountains\n",
      "\n",
      "### Paragraph:\n",
      "India experienced a crisis over Hyderabad from 21 August to 18 September 1948. Hyderabad was the second largest princely state in the subcontinent, after Jammu and Kashmir. It is located in the geographic center of India and, in 1948, 80 percent of its population was Hindu while most of the political elite were Muslim. The nizam of Hyderabad did not favor accession to distant Pakistan.\n",
      "\n",
      "### Final Sentence:\n",
      "Nor did predominantly Hindu India attract him.\n",
      "\n",
      "### Assistant:\n"
     ]
    }
   ],
   "source": [
    "print(prompt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt_model, return_tensors=\"pt\").input_ids.cuda()\n",
    "#print(input_ids.shape) #Ok if we pass too long a prompt it goes OOM. We have to catch that. #2048\n",
    "assert(input_ids.shape[1]+250 < 2048 )\n",
    "output = model.generate(inputs=input_ids, temperature=temperature, max_new_tokens=max_new_tokens) #uh oh, 1966 was too big too. Do both the max tokens and prompt have to fit in the context window?\n",
    "answer = tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The final sentence to process is \"Nor did predominantly Hindu India attract him.\"\n",
      "The references in that final sentence are \"India\" and \"Hindu India\".\n",
      "\"India\" directly refers to the country \"India\" and so include \"India\" in the list.\n",
      "\"Hindu India\" indirectly refers to the religion \"Hinduism\" and so include \"Hinduism\" in the list.\n",
      "\n",
      "### Final Answer:\n",
      "India; Hinduism</s>\n"
     ]
    }
   ],
   "source": [
    "print(answer.replace(prompt,'').replace('<s>### Human: \\n','').replace('<s> ### Human: ','').replace('\\n### Assistant:',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "max_new_tokens=250\n",
    "temperature=0.7\n",
    "\n",
    "ICBeLLM_xy = pd.read_csv(\"/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/ICBeLLM_xy.tsv\", sep=\"\\t\", na_values='').reset_index(drop=True).fillna('')\n",
    "ICBeLLM_xy.story_so_far = ICBeLLM_xy.story_so_far.fillna('')\n",
    "file=\"/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/\" + variable_name + \".tsv\"\n",
    "if os.path.exists(file):\n",
    "    temp=pd.read_csv(file,sep=\"\\t\", na_values='').fillna('')\n",
    "else:\n",
    "    temp=ICBeLLM_xy.copy()\n",
    "    temp.loc[:,'model']=quantized_model_dir\n",
    "    temp.loc[:,'max_new_tokens']=max_new_tokens\n",
    "    temp.loc[:,'temperature']=temperature\n",
    "    temp.loc[:,'variable']=variable_name\n",
    "    temp.loc[:,'time'] = None\n",
    "    temp.loc[:,'n_tokens'] = None\n",
    "    temp.loc[:,'prompt']=None\n",
    "    temp.loc[:,'response']=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens=250\n",
    "i=1\n",
    "prompt = prompt_base + temp['story_so_far'].values[i] + \"\\n\\n### Final Sentence:\\n\" + \"\\n\"\n",
    "prompt_model = f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "tokens_remaining = 2048 - max_new_tokens - len(prompt_model)/4\n",
    "print(tokens_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "characters_remaining = np.round((tokens_remaining*4))\n",
    "#print(characters_remaining)\n",
    "n=len(temp['sentence_span_text'].values[i])\n",
    "#print(n)\n",
    "start=max(n-characters_remaining, 0)\n",
    "temp['sentence_span_text'].values[i][start:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "195-60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = prompt_base + temp['story_so_far'].values[i] + \"\\n\\n### Final Sentence:\\n\" + temp['sentence_span_text'].values[i][-(tokens_remaining*4):] + \"\\n\"\n",
    "#prompt_model = f'''### Human: {prompt}\n",
    "#### Assistant:'''\n",
    "#\n",
    "#print(prompt_model)\n",
    "#input_ids = tokenizer(prompt_model, return_tensors=\"pt\").input_ids.cuda()\n",
    "#print(input_ids.shape) #Ok if we pass too long a prompt it goes OOM. We have to catch that. #2048\n",
    "#assert(input_ids.shape[0]+250 < 2048 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2358\n",
      "861\n",
      "1982\n"
     ]
    }
   ],
   "source": [
    "i=618\n",
    "int(i)\n",
    "prompt = prompt_base + temp['sentence_span_text'].values[i] + \"\\n\\n### Final Sentence:\\n\" + \"\\n\"\n",
    "prompt_model = f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "tokens_remaining = 2048 - max_new_tokens - len(prompt_model)/4 - 150 #750 #150 #take off another buffer 150 characters just to be safe\n",
    "characters_remaining = np.round((tokens_remaining*4))\n",
    "n=len(temp['story_so_far'].values[i])\n",
    "print(n)\n",
    "start= int(max(n-characters_remaining, 0))\n",
    "print(start)\n",
    "prompt = prompt_base + \"\\n\" + temp['story_so_far'].values[i][start:n] + \"\\n\\n### Final Sentence:\\n\" + temp['sentence_span_text'].values[i] + \"\\n\"\n",
    "prompt_model = f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "input_ids = tokenizer(prompt_model, return_tensors=\"pt\").input_ids.cuda()\n",
    "print(input_ids.shape[1]+max_new_tokens)\n",
    "assert(input_ids.shape[1]+max_new_tokens < 2048 ) #I'm getting OOM errors at least at 1961, 1629 is ok\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=max_new_tokens) #uh oh, 1966 was too big too. Do both the max tokens and prompt have to fit in the context window?\n",
    "answer = tokenizer.decode(output[0])\n",
    "temp.loc[i,'prompt'] = prompt_model\n",
    "temp.loc[i,'response'] = answer.replace(prompt_model,'')\n",
    "\n",
    "#temp.to_csv(file, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1982"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_ids.shape[1] + max_new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "#65 fails\n",
    "#Grab the last 2k characters of the story to avoid oom\n",
    "for i in range(temp.shape[0]):\n",
    "    if pd.isna(temp['response'][i]) or temp['response'][i]=='' :\n",
    "        start_time = timeit.default_timer()\n",
    "        #print(i)\n",
    "        prompt = prompt_base + temp['sentence_span_text'].values[i] + \"\\n\\n### Final Sentence:\\n\" + \"\\n\"\n",
    "        prompt_model = f'''### Human: {prompt}\n",
    "        ### Assistant:'''\n",
    "        tokens_remaining = 2048 - max_new_tokens - len(prompt_model)/4 - 150 #take off 150 just to be safe\n",
    "        characters_remaining = np.round((tokens_remaining*4))\n",
    "        n=len(temp['story_so_far'].values[i])\n",
    "        start= int(max(n-characters_remaining, 0))\n",
    "        prompt = prompt_base + \"\\n\" + temp['story_so_far'].values[i][start:n] + \"\\n\\n### Final Sentence:\\n\" + temp['sentence_span_text'].values[i] + \"\\n\"\n",
    "        prompt_model = f'''### Human: {prompt}\n",
    "        ### Assistant:'''\n",
    "        input_ids = tokenizer(prompt_model, return_tensors=\"pt\").input_ids.cuda()\n",
    "        assert(input_ids.shape[1]+max_new_tokens < 2048 )\n",
    "        output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=max_new_tokens) #uh oh, 1966 was too big too. Do both the max tokens and prompt have to fit in the context window?\n",
    "        answer = tokenizer.decode(output[0])\n",
    "        stopt_time = timeit.default_timer() \n",
    "        temp.loc[i,'time'] = round(stopt_time- start_time,3)\n",
    "        temp.loc[i,'n_tokens']=input_ids.shape[1]\n",
    "        temp.loc[i,'prompt'] = prompt_model\n",
    "        temp.loc[i,'response'] = answer.replace(prompt_model,'')\n",
    "        \n",
    "        temp.to_csv(file, sep=\"\\t\", index=False)\n",
    "        #torch.cuda.empty_cache()\n",
    "\n",
    "#ICBeLLM_xy['prompt_entity_list_explicit']\n",
    "#[prompt]\n",
    "#ICBeLLM_xy['story_so_far'].fillna('').values[i] \n",
    "#pd.isna(temp['yhat_prompt_entity_yes_no'][1])\n",
    "#temp['yhat_prompt_entity_yes_no'][0] = \"asdasdasd\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "618"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i #618\n",
    "#temp['story_so_far'].values[i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
