{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ/discussions/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece\n",
    "#!pip install tokenizers -U\n",
    "#!pip install transformers[sentencepiece] -U\n",
    "#!pip install accelerate -U\n",
    "#!pip install optimum\n",
    "#!pip install GPUtil\n",
    "#https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ/discussions/5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_name = \"yhat_countries\"\n",
    "import pandas as pd\n",
    "import os\n",
    "max_new_tokens=200\n",
    "temperature=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/skynet3/8tb_a/rwd_github_private/ICBeLLM'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#https://stackoverflow.com/questions/73747731/runtimeerror-cuda-out-of-memory-how-setting-max-split-size-mb\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\" #setting this very small finally solved my OOM error\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "cwd\n",
    "os.chdir('/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/')\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 31% | 10% |\n",
      "|  1 |  0% |  0% |\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" #auto gets confused for some reason and puts it on the 2080\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True #https://blog.paperspace.com/pytorch-memory-multi-gpu-debugging/\n",
    "torch.backends.cudnn.enabled = True #https://blog.paperspace.com/pytorch-memory-multi-gpu-debugging/\n",
    "torch.cuda.device_count()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(device=None)\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import GPUtil\n",
    "GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skynet3/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "\n",
    "# If you've already downloaded the model, reference its location here:\n",
    "#quantized_model_dir = \"/path/to/TheBloke_WizardLM-Uncensored-Falcon-40B-3bit-GPTQ\"\n",
    "# Or to download it from the hub and store it in the Hugging Face cache directory:\n",
    "\n",
    "#quantized_model_dir = \"./data_temp/Wizard-Vicuna-13B-Uncensored-GPTQ/\" #TheBloke_WizardLM-Uncensored-Falcon-40B-3bit-GPTQ\n",
    "#quantized_model_dir = \"TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ\"\n",
    "quantized_model_dir = \"TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ\"\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The safetensors archive passed at /home/skynet3/.cache/huggingface/hub/models--TheBloke--Wizard-Vicuna-30B-Uncensored-GPTQ/snapshots/7aeb8db6d49732afa76dc7ce60c5c040dbbcf3ea/Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    }
   ],
   "source": [
    "#FileNotFoundError: Could not find model in TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ\n",
    "#https://huggingface.co/TheBloke/wizard-vicuna-13B-GPTQ/discussions/7\n",
    "#quantized_model_dir = \"TheBloke/stable-vicuna-13B-GPTQ\"\n",
    "#model_basename = \"wizard-vicuna-13B-GPTQ-4bit.compat.no-act-order\" #YOU HAVE TO DO THIS\n",
    "\n",
    "#model_basename = \"Wizard-vicuna-30B-Uncensored-GPTQ-4bit.act-order\"\n",
    "# path to directory containing local model\n",
    "#https://huggingface.co/TheBloke/starcoderplus-GPTQ/discussions/3\n",
    "quantized_model_dir = \"TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ\" #\"/workspace/models/TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ\"\n",
    "model_basename = \"Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order\" #.safetensors\n",
    "use_triton = False \n",
    "\n",
    "#https://github.com/PanQiWei/AutoGPTQ\n",
    "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, \n",
    "                                           device=\"cuda:0\", \n",
    "                                           use_triton=False,\n",
    "                                           use_safetensors=True , \n",
    "                                           model_basename=model_basename,\n",
    "                                           quantize_config=None,\n",
    "                                           #torch_dtype=torch.bfloat16, \n",
    "                                           trust_remote_code=True\n",
    "                                           ) #\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hf_device_map #yup, it offloaded the last layers to the CPU. That's why it's so slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Output ***\n",
      "<s> ### Human: Ask Something here\n",
      "### Assistant: Sure, I can help you with anything related to the topic of this book. What would you like to know?</s>\n",
      "**************\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Ask Something here\"\n",
    "prompt_model = f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "input_ids = tokenizer(prompt_model, return_tensors=\"pt\").input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "\n",
    "print(\"*** Output ***\")\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(\"**************\")\n",
    "del output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok it continues to be about 4 characters per token\n",
    "# print(input_ids.shape) #So this prompt and a little bit of story is about 939 tokens, got another 1k to play with\n",
    "#print(len(prompt))\n",
    "#5046/1287\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1418])\n"
     ]
    }
   ],
   "source": [
    "prompt_base = \"\"\"Create a semicolon separated list of countries and coalitions of countries mentioned in final sentence of the paragraph. If necessary, first perform coreference resolution and map ambiguous pronouns or references to named entities mentioned earlier in the paragraph. If an entity such as a leader or a military unit represents a country then include the country that entity represents instead of the entity. Think step by step. Explain your reasoning. Be concise. Give a final answer.\n",
    "\n",
    "### Paragraph:\n",
    "The Communist regime, which had attained power in Russia on 7 November 1917, opted to withdraw from World War I through a separate peace with Germany--the Treaty of Brest-Litovsk on 3 March 1918.\n",
    "\n",
    "### Final Sentence:\n",
    "The Western Allies were anxious to maintain an eastern front against Germany and wished to prevent the Germans from seizing large stocks of arms in Vladivostok.\n",
    "\n",
    "### Thought:\n",
    "The entities mentioned in the final sentence are \"Western Allies\", \"Germany\", and \"Vladivostok\".\n",
    "\"Western Allies\" refers to a coalition of countries who are not explicitly listed yet in the paragraph and so Western Allies should be included.\n",
    "\"Germany\" refers to a country and so Germany should be included.\n",
    "\"Vladivostok\" refers to a city in Russia and so Russia should be included.\n",
    "\n",
    "### Final Answer:\n",
    "Western Allies; Germany; Russia\n",
    "\n",
    "### Paragraph:\n",
    "Three of the Austro-Hungarian Empire's successor states--Czechoslovakia, Yugoslavia, and Hungary--experienced a two-phase crisis from 27 March until 15 November 1921. Karl IV, the last Hapsburg Emperor of Austria-Hungary, abdicated in November 1918 and was exiled to Switzerland; but he did not abdicate his right to the throne. Early in 1921 the Conference of Ambassadors, acting for the Supreme Council (Big Ten) at the Paris Peace Conference, issued a decree forbidding the restoration of the Hapsburg family to the vacant Hungarian throne. Moreover, several European governments issued statements indicating what recourse they might have if a Hapsburg reascended the throne in Budapest. On 27 March 1921 Karl unexpectedly returned to his homeland and called upon the Regent of Hungary, Admiral Horthy, to transfer his powers to the rightful ruler. This act triggered a crisis for Czechoslovakia and Yugoslavia.\n",
    "\n",
    "### Final Sentence:\n",
    "The latter responded two days later by declaring that it would regard any attempt to restore Karl as a casus belli.\n",
    "\n",
    "### Thought:\n",
    "The entities mentioned in the final sentence are \"The latter\", an implied actor who might \"attempt to restore Karl\", and \"Karl\".\n",
    "\"The latter' refers to Yugoslavia from the previous sentence which is a country and so Yugoslavia should be included.\n",
    "\"attempt to restore Karl\" refers to Hungary which is a country and so Hungary should be included.\n",
    "\n",
    "### Final Answer:\n",
    "Yugoslavia; Hungary\n",
    "\n",
    "### Paragraph:\n",
    "Cuba and the United States were the adversaries in a crisis from 15 to 24 April 1961. A series of incursions into Caribbean and Central American states, in which Cuba was perceived to be involved, took place in 1959-60 In addition, the U.S. had become increasingly concerned about the strong ties that had developed between Cuba and the USSR. Relations between the U.S. and Cuba were strained; and the latter appealed to the Security Council at the end of 1960 charging the U.S. with plans to mount an invasion of Cuba using Cuban exiles trained in Guatemala. Diplomatic relations between the two countries were broken on 3 January 1961. Cuba and the United States were the adversaries in a crisis from 15 to 24 April 1961. A series of incursions into Caribbean and Central American states, in which Cuba was perceived to be involved, took place in 1959-60 In addition, the U.S. had become increasingly concerned about the strong ties that had developed between Cuba and the USSR. Relations between the U.S. and Cuba were strained; and the latter appealed to the Security Council at the end of 1960 charging the U.S. with plans to mount an invasion of Cuba using Cuban exiles trained in Guatemala. Diplomatic relations between the two countries were broken on 3 January 1961.\n",
    "\n",
    "### Final Sentence:\n",
    "In early April 1961 President Kennedy authorized an invasion of Cuba by the exiles, reasoning that, in the event of a failure, the remaining force would be able to establish guerrilla bases in the Cuban mountains.\n",
    "\n",
    "### Thought: \n",
    "The entities mentioned in the final sentence are \"President Kennedy\", \"Cuba\", \"the exiles\", and \"the Cuban mountains\".\n",
    "\"President Kennedy\" refers to President John F. Kennedy who is leader who represents the United States which is a country and so United States should be included.\n",
    "\"Cuba\" is a country and so Cuba should be included.\n",
    "\"The exiles\" refers to Cuban Exiles which is independent group and not a country or representing a country and so should be excluded.\n",
    "\"The Cuban mountains\" refers to a geographic feature that is part of Cuba and so Cuba should be included.\n",
    "\n",
    "### Final Answer:\n",
    "United States; Cuba\n",
    "\n",
    "### Paragraph:\"\"\"\n",
    "\n",
    "this_obs = \"\"\"\n",
    "India experienced a crisis over Hyderabad from 21 August to 18 September 1948. Hyderabad was the second largest princely state in the subcontinent, after Jammu and Kashmir. It is located in the geographic center of India and, in 1948, 80 percent of its population was Hindu while most of the political elite were Muslim. The nizam of Hyderabad did not favor accession to distant Pakistan.\n",
    "\n",
    "### Final Sentence:\n",
    "Nor did predominantly Hindu India attract him.\n",
    "\"\"\"\n",
    "prompt = prompt_base + this_obs\n",
    "prompt_model = f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "#print(tokenizer.decode(model.generate(**tokenizer(prompt_model, return_tensors=\"pt\").to(model.device))[0]))\n",
    "\n",
    "input_ids = tokenizer(prompt_model, return_tensors=\"pt\").input_ids.cuda()\n",
    "print(input_ids.shape) #Ok if we pass too long a prompt it goes OOM. We have to catch that. #2048\n",
    "assert(input_ids.shape[0]+250 < 2048 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: Create a semicolon separated list of countries and coalitions of countries mentioned in final sentence of the paragraph. If necessary, first perform coreference resolution and map ambiguous pronouns or references to named entities mentioned earlier in the paragraph. If an entity such as a leader or a military unit represents a country then include the country that entity represents instead of the entity. Think step by step. Explain your reasoning. Be concise. Give a final answer.\n",
      "\n",
      "### Paragraph:\n",
      "The Communist regime, which had attained power in Russia on 7 November 1917, opted to withdraw from World War I through a separate peace with Germany--the Treaty of Brest-Litovsk on 3 March 1918.\n",
      "\n",
      "### Final Sentence:\n",
      "The Western Allies were anxious to maintain an eastern front against Germany and wished to prevent the Germans from seizing large stocks of arms in Vladivostok.\n",
      "\n",
      "### Thought:\n",
      "The entities mentioned in the last sentence are \"Western Allies\", \"Germany\", and \"Vladivostok\".\n",
      "\"Western Allies\" refers to a coalition of countries who are not explicitly listed yet in the paragraph and so Western Allies should be included.\n",
      "\"Germany\" refers to a country and so Germany should be included.\n",
      "\"Vladivostok\" refers to a city in Russia and so Russia should be included.\n",
      "\n",
      "### Final Answer:\n",
      "Western Allies; Germany; Russia\n",
      "\n",
      "### Paragraph:\n",
      "Three of the Austro-Hungarian Empire's successor states--Czechoslovakia, Yugoslavia, and Hungary--experienced a two-phase crisis from 27 March until 15 November 1921. Karl IV, the last Hapsburg Emperor of Austria-Hungary, abdicated in November 1918 and was exiled to Switzerland; but he did not abdicate his right to the throne. Early in 1921 the Conference of Ambassadors, acting for the Supreme Council (Big Ten) at the Paris Peace Conference, issued a decree forbidding the restoration of the Hapsburg family to the vacant Hungarian throne. Moreover, several European governments issued statements indicating what recourse they might have if a Hapsburg reascended the throne in Budapest. On 27 March 1921 Karl unexpectedly returned to his homeland and called upon the Regent of Hungary, Admiral Horthy, to transfer his powers to the rightful ruler. This act triggered a crisis for Czechoslovakia and Yugoslavia.\n",
      "\n",
      "### Final Sentence:\n",
      "The latter responded two days later by declaring that it would regard any attempt to restore Karl as a casus belli.\n",
      "\n",
      "### Thought:\n",
      "The entities mentioned in the last sentence are \"The latter\", an implied actor who might \"attempt to restore Karl\", and \"Karl\".\n",
      "\"The latter' refers to Yugoslavia from the previous sentence which is a country and so Yugoslavia should be included.\n",
      "\"Any attempt to restore Karl\" refers to Hungary which is a country and so Hungary should be included.\n",
      "\n",
      "### Final Answer:\n",
      "Yugoslavia; Hungary\n",
      "\n",
      "### Paragraph:\n",
      "Cuba and the United States were the adversaries in a crisis from 15 to 24 April 1961. A series of incursions into Caribbean and Central American states, in which Cuba was perceived to be involved, took place in 1959-60 In addition, the U.S. had become increasingly concerned about the strong ties that had developed between Cuba and the USSR. Relations between the U.S. and Cuba were strained; and the latter appealed to the Security Council at the end of 1960 charging the U.S. with plans to mount an invasion of Cuba using Cuban exiles trained in Guatemala. Diplomatic relations between the two countries were broken on 3 January 1961. Cuba and the United States were the adversaries in a crisis from 15 to 24 April 1961. A series of incursions into Caribbean and Central American states, in which Cuba was perceived to be involved, took place in 1959-60 In addition, the U.S. had become increasingly concerned about the strong ties that had developed between Cuba and the USSR. Relations between the U.S. and Cuba were strained; and the latter appealed to the Security Council at the end of 1960 charging the U.S. with plans to mount an invasion of Cuba using Cuban exiles trained in Guatemala. Diplomatic relations between the two countries were broken on 3 January 1961.\n",
      "\n",
      "### Final Sentence:\n",
      "In early April 1961 President Kennedy authorized an invasion of Cuba by the exiles, reasoning that, in the event of a failure, the remaining force would be able to establish guerrilla bases in the Cuban mountains.\n",
      "\n",
      "### Thought: \n",
      "The entities mentioned in that last sentence are \"President Kennedy\", \"Cuba\", \"the exiles\", and \"the Cuban mountains\".\n",
      "\"President Kennedy\" refers to President John F. Kennedy who is leader who represents the United States which is a country and so United States should be included.\n",
      "\"Cuba\" is a country and so Cuba should be included.\n",
      "\"The exiles\" refers to Cuban Exiles which is independent group and not a country or representing a country and so should be excluded.\n",
      "\"The Cuban mountains\" refers to a geographic feature that is part of Cuba and so Cuba should be included.\n",
      "\n",
      "### Final Answer:\n",
      "United States; Cuba\n",
      "\n",
      "### Paragraph:\n",
      "India experienced a crisis over Hyderabad from 21 August to 18 September 1948. Hyderabad was the second largest princely state in the subcontinent, after Jammu and Kashmir. It is located in the geographic center of India and, in 1948, 80 percent of its population was Hindu while most of the political elite were Muslim. The nizam of Hyderabad did not favor accession to distant Pakistan.\n",
      "\n",
      "### Final Sentence:\n",
      "Nor did predominantly Hindu India attract him.\n",
      "\n",
      "### Assistant:\n"
     ]
    }
   ],
   "source": [
    "print(prompt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skynet3/anaconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>auto_gptq is a tool for automatically generating Python code for Google's Cloud Trans\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens=250\n",
    "temperature=0.7\n",
    "output = model.generate(inputs=input_ids, temperature=temperature, max_new_tokens=max_new_tokens) #uh oh, 1966 was too big too. Do both the max tokens and prompt have to fit in the context window?\n",
    "answer = tokenizer.decode(output[0])\n",
    "\n",
    "print(tokenizer.decode(model.generate(**tokenizer(\"auto_gptq is\", return_tensors=\"pt\").to(model.device))[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The entities mentioned in the last sentence are \"Nizam of Hyderabad\" and \"India\".\n",
      "\"Nizam of Hyderabad\" refers to a leader who represents Hyderabad and so Hyderabad should be included.\n",
      "\"India\" is a country and so India should be included.\n",
      "\n",
      "### Final Answer:\n",
      "Hyderabad; India</s>\n"
     ]
    }
   ],
   "source": [
    "print(answer.replace(prompt,'').replace('<s>### Human: \\n','').replace('<s> ### Human: ','').replace('\\n### Assistant:',''))\n",
    "del output , answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ICBeLLM_xy = pd.read_csv(\"/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/ICBeLLM_xy.tsv\", sep=\"\\t\", na_values='').reset_index(drop=True).fillna('')\n",
    "ICBeLLM_xy.story_so_far = ICBeLLM_xy.story_so_far.fillna('')\n",
    "file=\"/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/\" + variable_name + \".tsv\"\n",
    "if os.path.exists(file):\n",
    "    temp=pd.read_csv(file,sep=\"\\t\", na_values='').fillna('')\n",
    "else:\n",
    "    temp=ICBeLLM_xy.copy()\n",
    "    temp.loc[:,'model']=quantized_model_dir\n",
    "    temp.loc[:,'max_new_tokens']=max_new_tokens\n",
    "    temp.loc[:,'temperature']=temperature\n",
    "    temp.loc[:,'variable']=variable_name\n",
    "    temp.loc[:,'time'] = None\n",
    "    temp.loc[:,'n_tokens'] = None\n",
    "    temp.loc[:,'prompt']=None\n",
    "    temp.loc[:,'response']=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "533.0\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "prompt = prompt_base + temp['story_so_far'].values[i] + \"\\n\\n### Final Sentence:\\n\" + \"\\n\"\n",
    "prompt_model = f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "tokens_remaining = 2048 - max_new_tokens - len(prompt_model)/4\n",
    "print(tokens_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Communist regime, which had attained power in Russia on 7 November 1917, opted to withdraw from World War I through a separate peace with Germany--the Treaty of Brest-Litovsk on 3 March 1918.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "characters_remaining = np.round((tokens_remaining*4))\n",
    "#print(characters_remaining)\n",
    "n=len(temp['sentence_span_text'].values[i])\n",
    "#print(n)\n",
    "start=max(n-characters_remaining, 0)\n",
    "temp['sentence_span_text'].values[i][start:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = prompt_base + temp['story_so_far'].values[i] + \"\\n\\n### Final Sentence:\\n\" + temp['sentence_span_text'].values[i][-(tokens_remaining*4):] + \"\\n\"\n",
    "#prompt_model = f'''### Human: {prompt}\n",
    "#### Assistant:'''\n",
    "#\n",
    "#print(prompt_model)\n",
    "#input_ids = tokenizer(prompt_model, return_tensors=\"pt\").input_ids.cuda()\n",
    "#print(input_ids.shape) #Ok if we pass too long a prompt it goes OOM. We have to catch that. #2048\n",
    "#assert(input_ids.shape[0]+250 < 2048 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2754\n",
      "1442\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 26% | 10% |\n",
      "|  1 |  0% | 97% |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>### Human: Create a semicolon separated list of countries and coalitions of countries mentioned in final sentence of the paragraph. If necessary, first perform coreference resolution and map ambiguous pronouns or references to named entities mentioned earlier in the paragraph. If an entity such as a leader or a military unit represents a country then include the country that entity represents instead of the entity. Think step by step. Explain your reasoning. Be concise. Give a final answer.\n",
      "\n",
      "### Paragraph:\n",
      "The Communist regime, which had attained power in Russia on 7 November 1917, opted to withdraw from World War I through a separate peace with Germany--the Treaty of Brest-Litovsk on 3 March 1918.\n",
      "\n",
      "### Final Sentence:\n",
      "The Western Allies were anxious to maintain an eastern front against Germany and wished to prevent the Germans from seizing large stocks of arms in Vladivostok.\n",
      "\n",
      "### Thought:\n",
      "The entities mentioned in the last sentence are \"Western Allies\", \"Germany\", and \"Vladivostok\".\n",
      "\"Western Allies\" refers to a coalition of countries who are not explicitly listed yet in the paragraph and so Western Allies should be included.\n",
      "\"Germany\" refers to a country and so Germany should be included.\n",
      "\"Vladivostok\" refers to a city in Russia and so Russia should be included.\n",
      "\n",
      "### Final Answer:\n",
      "Western Allies; Germany; Russia\n",
      "\n",
      "### Paragraph:\n",
      "Three of the Austro-Hungarian Empire's successor states--Czechoslovakia, Yugoslavia, and Hungary--experienced a two-phase crisis from 27 March until 15 November 1921. Karl IV, the last Hapsburg Emperor of Austria-Hungary, abdicated in November 1918 and was exiled to Switzerland; but he did not abdicate his right to the throne. Early in 1921 the Conference of Ambassadors, acting for the Supreme Council (Big Ten) at the Paris Peace Conference, issued a decree forbidding the restoration of the Hapsburg family to the vacant Hungarian throne. Moreover, several European governments issued statements indicating what recourse they might have if a Hapsburg reascended the throne in Budapest. On 27 March 1921 Karl unexpectedly returned to his homeland and called upon the Regent of Hungary, Admiral Horthy, to transfer his powers to the rightful ruler. This act triggered a crisis for Czechoslovakia and Yugoslavia.\n",
      "\n",
      "### Final Sentence:\n",
      "The latter responded two days later by declaring that it would regard any attempt to restore Karl as a casus belli.\n",
      "\n",
      "### Thought:\n",
      "The entities mentioned in the last sentence are \"The latter\", an implied actor who might \"attempt to restore Karl\", and \"Karl\".\n",
      "\"The latter' refers to Yugoslavia from the previous sentence which is a country and so Yugoslavia should be included.\n",
      "\"Any attempt to restore Karl\" refers to Hungary which is a country and so Hungary should be included.\n",
      "\n",
      "### Final Answer:\n",
      "Yugoslavia; Hungary\n",
      "\n",
      "### Paragraph:\n",
      "Cuba and the United States were the adversaries in a crisis from 15 to 24 April 1961. A series of incursions into Caribbean and Central American states, in which Cuba was perceived to be involved, took place in 1959-60 In addition, the U.S. had become increasingly concerned about the strong ties that had developed between Cuba and the USSR. Relations between the U.S. and Cuba were strained; and the latter appealed to the Security Council at the end of 1960 charging the U.S. with plans to mount an invasion of Cuba using Cuban exiles trained in Guatemala. Diplomatic relations between the two countries were broken on 3 January 1961. Cuba and the United States were the adversaries in a crisis from 15 to 24 April 1961. A series of incursions into Caribbean and Central American states, in which Cuba was perceived to be involved, took place in 1959-60 In addition, the U.S. had become increasingly concerned about the strong ties that had developed between Cuba and the USSR. Relations between the U.S. and Cuba were strained; and the latter appealed to the Security Council at the end of 1960 charging the U.S. with plans to mount an invasion of Cuba using Cuban exiles trained in Guatemala. Diplomatic relations between the two countries were broken on 3 January 1961.\n",
      "\n",
      "### Final Sentence:\n",
      "In early April 1961 President Kennedy authorized an invasion of Cuba by the exiles, reasoning that, in the event of a failure, the remaining force would be able to establish guerrilla bases in the Cuban mountains.\n",
      "\n",
      "### Thought: \n",
      "The entities mentioned in that last sentence are \"President Kennedy\", \"Cuba\", \"the exiles\", and \"the Cuban mountains\".\n",
      "\"President Kennedy\" refers to President John F. Kennedy who is leader who represents the United States which is a country and so United States should be included.\n",
      "\"Cuba\" is a country and so Cuba should be included.\n",
      "\"The exiles\" refers to Cuban Exiles which is independent group and not a country or representing a country and so should be excluded.\n",
      "\"The Cuban mountains\" refers to a geographic feature that is part of Cuba and so Cuba should be included.\n",
      "\n",
      "### Final Answer:\n",
      "United States; Cuba\n",
      "\n",
      "### Paragraph:\n",
      ". No serious military response was attempted. On the 24th the Japanese government called for direct Sino/Japanese negotiations, while rejecting involvement by the League. At the same time Japanese military pressure continued, and the northeastern provinces of China were steadily occupied. The Chinese instituted a boycott of Japanese imports into the country. The crisis ended for Japan and, by force majeure, for China on 18 February 1932, when Japan unilaterally declared the independence of Manchuria, now called Manchukuo. The League of Nations adopted a resolution on 10 December 1931 to dispatch a fact-finding mission. After the end of the crisis the Lytton Commission gently chided Japan: it found that the Japanese action of 18-19 September 1931 was not in self-defense and that the creation of Manchukuo did not flow from a \"\"genuine and spontaneous independence movement.\"\" The Commission's criticism of Japan's behavior was adopted by the League Assembly, leading to Japan's notice of withdrawal from the League in February 1933. The United States condemned Japan's behavior but confined its action to nonrecognition of Manchukuo. Japan viewed the U.S. with hostility and suspicion, while China was dissatisfied with the paucity of the U.S. response. The USSR proposed a nonaggression pact to Japan.\n",
      "\n",
      "### Final Sentence:\n",
      "Territory previously recognized by Japan to be in the Soviet sphere of influence was now included in Manchukuo as the new state expanded its borders; but Moscow assumed Chinese rights to the Chinese Eastern Railway, which proved to be a continuing source of friction.\n",
      "\n",
      "### Assistant:\n",
      "The entities mentioned in the last sentence are \"Moscow\", \"Chinese Eastern Railway\", and \"China\".\n",
      "\"Moscow\" refers to the Soviet Union which is a country and so Soviet Union should be included.\n",
      "\"Chinese Eastern Railway\" refers to a railway that was built by Russia and later transferred to China and so China should be included.\n",
      "\n",
      "### Final Answer:\n",
      "Soviet Union; China\n",
      "\n",
      "### Paragraph:\n",
      "The crisis between the United States and Iran over the latter's nuclear program escalated in 2012. The U.S. and its allies imposed economic sanctions on Iran, while Iran threatened to close the Strait of Hormuz, a vital oil shipping lane. The crisis was resolved in 2015 with the signing of the Joint Comprehensive Plan of Action (JCPOA) between Iran and the P5+1 (China, France, Russia, the United Kingdom, and the United States), which limited Iran's nuclear program in exchange for the lifting of sanctions. However, the U.S. withdrew from the JCPOA in\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "i=880\n",
    "int(i)\n",
    "prompt = prompt_base + temp['sentence_span_text'].values[i] + \"\\n\\n### Final Sentence:\\n\" + \"\\n\"\n",
    "prompt_model = f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "tokens_remaining = 2048 - max_new_tokens - len(prompt_model)/4 - 150 #750 #150 #take off another buffer 150 characters just to be safe\n",
    "characters_remaining = np.round((tokens_remaining*4))\n",
    "n=len(temp['story_so_far'].values[i])\n",
    "print(n)\n",
    "start= int(max(n-characters_remaining, 0))\n",
    "print(start)\n",
    "prompt = prompt_base + \"\\n\" + temp['story_so_far'].values[i][start:n] + \"\\n\\n### Final Sentence:\\n\" + temp['sentence_span_text'].values[i] + \"\\n\"\n",
    "prompt_model = f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "#input_ids = tokenizer(prompt_model, return_tensors=\"pt\").input_ids.cuda()\n",
    "#print(input_ids.shape[1]+max_new_tokens)\n",
    "#assert(input_ids.shape[1]+max_new_tokens < 2048 ) #I'm getting OOM errors at least at 1961, 1629 is ok\n",
    "#output = model.generate(inputs=input_ids, temperature=temperature, max_new_tokens=max_new_tokens).to('cpu')  #adding to cpu here hoping that helps #uh oh, 1966 was too big too. Do both the max tokens and prompt have to fit in the context window?\n",
    "#answer = tokenizer.decode(output[0])\n",
    "GPUtil.showUtilization()\n",
    "with torch.no_grad(): #https://blog.paperspace.com/pytorch-memory-multi-gpu-debugging/\n",
    "    print(tokenizer.decode(model.generate(**tokenizer(prompt_model, return_tensors=\"pt\").to(model.device), max_new_tokens=max_new_tokens)[0]))\n",
    "#temp.loc[i,'prompt'] = prompt_model\n",
    "#temp.loc[i,'response'] = answer.replace(prompt_model,'')\n",
    "\n",
    "#temp.to_csv(file, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1668"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape[1] + max_new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m prompt_model \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'''\u001b[39m\u001b[39m### Human: \u001b[39m\u001b[39m{\u001b[39;00mprompt\u001b[39m}\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m### Assistant:\u001b[39m\u001b[39m'''\u001b[39m\n\u001b[1;32m     19\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(prompt_model, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> 20\u001b[0m \u001b[39massert\u001b[39;00m(input_ids\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39mmax_new_tokens \u001b[39m<\u001b[39m \u001b[39m2048\u001b[39m )\n\u001b[1;32m     21\u001b[0m \u001b[39m#output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=max_new_tokens), 1966 was too big too. Do both the max tokens and prompt have to fit in the context window?\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m#answer = tokenizer.decode(output[0])\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m#Switched it to one full single line hoping it solves OOM error\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m#did not fix it\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad(): \u001b[39m#https://blog.paperspace.com/pytorch-memory-multi-gpu-debugging/\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "#65 fails\n",
    "#Grab the last 2k characters of the story to avoid oom\n",
    "for i in range(temp.shape[0]):\n",
    "    if pd.isna(temp['response'][i]) or temp['response'][i]=='' :\n",
    "        start_time = timeit.default_timer()\n",
    "        #print(i)\n",
    "        prompt = prompt_base + temp['sentence_span_text'].values[i] + \"\\n\\n### Final Sentence:\\n\" + \"\\n\"\n",
    "        prompt_model = f'''### Human: {prompt}\n",
    "        ### Assistant:'''\n",
    "        tokens_remaining = 2048 - max_new_tokens - len(prompt_model)/4 - 350 #take off 150 just to be safe\n",
    "        characters_remaining = np.round((tokens_remaining*4))\n",
    "        n=len(temp['story_so_far'].values[i])\n",
    "        start= int(max(n-characters_remaining, 0))\n",
    "        prompt = prompt_base + \"\\n\" + temp['story_so_far'].values[i][start:n] + \"\\n\\n### Final Sentence:\\n\" + temp['sentence_span_text'].values[i] + \"\\n\"\n",
    "        prompt_model = f'''### Human: {prompt}\n",
    "        ### Assistant:'''\n",
    "        input_ids = tokenizer(prompt_model, return_tensors=\"pt\").input_ids.cuda()\n",
    "        assert(input_ids.shape[1]+max_new_tokens < 2048 )\n",
    "        #output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=max_new_tokens), 1966 was too big too. Do both the max tokens and prompt have to fit in the context window?\n",
    "        #answer = tokenizer.decode(output[0])\n",
    "        #Switched it to one full single line hoping it solves OOM error\n",
    "        #did not fix it\n",
    "        with torch.no_grad(): #https://blog.paperspace.com/pytorch-memory-multi-gpu-debugging/\n",
    "            answer = tokenizer.decode(model.generate(**tokenizer(prompt_model, return_tensors=\"pt\").to(model.device), max_new_tokens=max_new_tokens, temperature=temperature )[0] )\n",
    "        stopt_time = timeit.default_timer() \n",
    "        temp.loc[i,'time'] = round(stopt_time- start_time,3)\n",
    "        temp.loc[i,'n_tokens']=input_ids.shape[1]\n",
    "        temp.loc[i,'prompt'] = prompt_model\n",
    "        temp.loc[i,'response'] = answer.replace(prompt_model,'')\n",
    "        del answer\n",
    "        temp.to_csv(file, sep=\"\\t\", index=False)\n",
    "        #torch.cuda.empty_cache()\n",
    "\n",
    "#ICBeLLM_xy['prompt_entity_list_explicit']\n",
    "#[prompt]\n",
    "#ICBeLLM_xy['story_so_far'].fillna('').values[i] \n",
    "#pd.isna(temp['yhat_prompt_entity_yes_no'][1])\n",
    "#temp['yhat_prompt_entity_yes_no'][0] = \"asdasdasd\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i #871\n",
    "#temp['story_so_far'].values[i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
