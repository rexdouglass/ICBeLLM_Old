{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support for ExLlama library #281\n",
    "https://github.com/microsoft/guidance/issues/281\n",
    "https://github.com/turboderp/exllama/pull/104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "import sys\n",
    "sys.path.insert(0, \"/home/skynet3/Downloads/exllama/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ExLlama, ExLlamaCache, ExLlamaConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import ExLlamaTokenizer\n",
    "from generator import ExLlamaGenerator\n",
    "import os, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directory containing model, tokenizer, generator\n",
    "\n",
    "#model_directory =  \"/mnt/str/models/llama-13b-4bit-128g/\"\n",
    "model_directory =  \"//media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_temp/Wizard-Vicuna-30B-Uncensored-GPTQ/\"\n",
    "\n",
    "# Locate files we need within that directory\n",
    "\n",
    "tokenizer_path = os.path.join(model_directory, \"tokenizer.model\")\n",
    "model_config_path = os.path.join(model_directory, \"config.json\")\n",
    "st_pattern = os.path.join(model_directory, \"*.safetensors\")\n",
    "model_path = glob.glob(st_pattern)[0]\n",
    "\n",
    "# Create config, model, tokenizer and generator\n",
    "\n",
    "config = ExLlamaConfig(model_config_path)               # create config from config.json\n",
    "config.model_path = model_path                          # supply path to model weights file\n",
    "\n",
    "model = ExLlama(config)                                 # create ExLlama instance and load the weights\n",
    "tokenizer = ExLlamaTokenizer(tokenizer_path)            # create tokenizer from tokenizer model file\n",
    "\n",
    "cache = ExLlamaCache(model)                             # create cache for inference\n",
    "generator = ExLlamaGenerator(model, tokenizer, cache)   # create generator\n",
    "\n",
    "# Configure generator\n",
    "\n",
    "generator.disallow_tokens([tokenizer.eos_token_id])\n",
    "\n",
    "generator.settings.token_repetition_penalty_max = 1.2\n",
    "generator.settings.temperature = 0.95\n",
    "generator.settings.top_p = 0.65\n",
    "generator.settings.top_k = 100\n",
    "generator.settings.typical = 0.5\n",
    "\n",
    "# Produce a simple generation\n",
    "\n",
    "prompt = \"Once upon a time,\"\n",
    "print (prompt, end = \"\")\n",
    "\n",
    "output = generator.generate_simple(prompt, max_new_tokens = 200)\n",
    "\n",
    "print(output[len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in the early days of television, there was a show called \"The Lone Ranger.\" The title character was a masked man who rode around on his horse, Silver, and fought for justice. He had a sidekick named Tonto, played by Jay Silverheels, who was a Native American.\n",
      "One day, while riding through the desert, the Lone Ranger came across an old prospector who was trapped under a boulder. Without hesitation, he jumped off his horse and used his strength to lift the rock and free the man. After thanking him profusely, the prospector offered the Lone Ranger some water from his canteen. As they sat together in the shade, the prospector told the Lone Ranger about a hidden gold mine that he had discovered years ago but never been able to find again.\n",
      "Determined to help the old man realize his dream, the Lone Ranger set out\n"
     ]
    }
   ],
   "source": [
    "output = generator.generate_simple(prompt, max_new_tokens = 200)\n",
    "\n",
    "print(output[len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidance\n",
    "#On how to do 4 bit, load the model like you would normally\n",
    "#https://github.com/microsoft/guidance/issues/43\n",
    "#https://github.com/microsoft/guidance/discussions/110\n",
    "llama = guidance.llms.Transformers(model=model, tokenizer=tokenizer, device=0)\n",
    "#guidance.llm = llama"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
