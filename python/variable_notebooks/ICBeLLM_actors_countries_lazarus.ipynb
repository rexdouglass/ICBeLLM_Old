{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ/discussions/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece\n",
    "#!pip install tokenizers -U\n",
    "#!pip install transformers[sentencepiece] -U\n",
    "#!pip install accelerate -U\n",
    "#!pip install optimum\n",
    "\n",
    "#https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ/discussions/5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/skynet3/8tb_a/rwd_github_private/ICBeLLM'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#https://stackoverflow.com/questions/73747731/runtimeerror-cuda-out-of-memory-how-setting-max-split-size-mb\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\" #experiment with this and the value, see if it prevents oom\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "cwd\n",
    "os.chdir('/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/')\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" #auto gets confused for some reason and puts it on the 2080\n",
    "import torch\n",
    "torch.cuda.device_count()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(device=None)\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skynet3/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "\n",
    "# If you've already downloaded the model, reference its location here:\n",
    "#quantized_model_dir = \"/path/to/TheBloke_WizardLM-Uncensored-Falcon-40B-3bit-GPTQ\"\n",
    "# Or to download it from the hub and store it in the Hugging Face cache directory:\n",
    "\n",
    "#quantized_model_dir = \"./data_temp/Wizard-Vicuna-13B-Uncensored-GPTQ/\" #TheBloke_WizardLM-Uncensored-Falcon-40B-3bit-GPTQ\n",
    "#quantized_model_dir = \"TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ\"\n",
    "quantized_model_dir = \"TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ\"\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The safetensors archive passed at /home/skynet3/.cache/huggingface/hub/models--TheBloke--Wizard-Vicuna-30B-Uncensored-GPTQ/snapshots/7aeb8db6d49732afa76dc7ce60c5c040dbbcf3ea/Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    }
   ],
   "source": [
    "#FileNotFoundError: Could not find model in TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ\n",
    "#https://huggingface.co/TheBloke/wizard-vicuna-13B-GPTQ/discussions/7\n",
    "#quantized_model_dir = \"TheBloke/stable-vicuna-13B-GPTQ\"\n",
    "#model_basename = \"wizard-vicuna-13B-GPTQ-4bit.compat.no-act-order\" #YOU HAVE TO DO THIS\n",
    "\n",
    "#model_basename = \"Wizard-vicuna-30B-Uncensored-GPTQ-4bit.act-order\"\n",
    "# path to directory containing local model\n",
    "#https://huggingface.co/TheBloke/starcoderplus-GPTQ/discussions/3\n",
    "quantized_model_dir = \"TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ\" #\"/workspace/models/TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ\"\n",
    "model_basename = \"Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order\" #.safetensors\n",
    "use_triton = False \n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, \n",
    "                                           device=\"cuda:0\", \n",
    "                                           use_triton=False,\n",
    "                                           use_safetensors=True , \n",
    "                                           model_basename=model_basename,\n",
    "                                           quantize_config=None,\n",
    "                                           #torch_dtype=torch.bfloat16, \n",
    "                                           trust_remote_code=True\n",
    "                                           ) #\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.hf_device_map #yup, it offloaded the last layers to the CPU. That's why it's so slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Output ***\n",
      "<s> ### Human: Ask Something here\n",
      "### Assistant: Sure, I can help you with anything related to the topic of this book. What would you like to know?</s>\n",
      "**************\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Ask Something here\"\n",
    "prompt_model = f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "input_ids = tokenizer(prompt_model, return_tensors=\"pt\").input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "\n",
    "print(\"*** Output ***\")\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(\"**************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok it continues to be about 4 characters per token\n",
    "# print(input_ids.shape) #So this prompt and a little bit of story is about 939 tokens, got another 1k to play with\n",
    "#print(len(prompt))\n",
    "#5046/1287\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_base = \"\"\"Create a semicolon separated list of countries and coalitions of countries mentioned in final sentence of the paragraph. If necessary, first perform coreference resolution and map ambiguous pronouns or references to named entities mentioned earlier in the paragraph. If an entity such as a leader or a military unit represents a country then include the country that entity represents instead of the entity. Explain your reasoning and then give a final answer.\n",
    "\n",
    "### Paragraph:\n",
    "The Communist regime, which had attained power in Russia on 7 November 1917, opted to withdraw from World War I through a separate peace with Germany--the Treaty of Brest-Litovsk on 3 March 1918.\n",
    "\n",
    "### Final Sentence:\n",
    "The Western Allies were anxious to maintain an eastern front against Germany and wished to prevent the Germans from seizing large stocks of arms in Vladivostok.\n",
    "\n",
    "### Thought:\n",
    "The entities mentioned in the last sentence are \"Western Allies\", \"Germany\", and \"Vladivostok\".\n",
    "\"Western Allies\" refers to a coalition of countries who are not explicitly listed yet in the paragraph and so Western Allies should be included.\n",
    "\"Germany\" refers to a country and so Germany should be included.\n",
    "\"Vladivostok\" refers to a city in Russia and so Russia should be included.\n",
    "\n",
    "### Final Answer:\n",
    "Western Allies; Germany; Russia\n",
    "\n",
    "### Paragraph:\n",
    "Three of the Austro-Hungarian Empire's successor states--Czechoslovakia, Yugoslavia, and Hungary--experienced a two-phase crisis from 27 March until 15 November 1921. Karl IV, the last Hapsburg Emperor of Austria-Hungary, abdicated in November 1918 and was exiled to Switzerland; but he did not abdicate his right to the throne. Early in 1921 the Conference of Ambassadors, acting for the Supreme Council (Big Ten) at the Paris Peace Conference, issued a decree forbidding the restoration of the Hapsburg family to the vacant Hungarian throne. Moreover, several European governments issued statements indicating what recourse they might have if a Hapsburg reascended the throne in Budapest. On 27 March 1921 Karl unexpectedly returned to his homeland and called upon the Regent of Hungary, Admiral Horthy, to transfer his powers to the rightful ruler. This act triggered a crisis for Czechoslovakia and Yugoslavia.\n",
    "\n",
    "### Final Sentence:\n",
    "The latter responded two days later by declaring that it would regard any attempt to restore Karl as a casus belli.\n",
    "\n",
    "### Thought:\n",
    "The entities mentioned in the last sentence are \"The latter\", an implied actor who might \"attempt to restore Karl\", and \"Karl\".\n",
    "\"The latter' refers to Yugoslavia from the previous sentence which is a country and so Yugoslavia should be included.\n",
    "\"Any attempt to restore Karl\" refers to Hungary which is a country and so Hungary should be included.\n",
    "\n",
    "### Final Answer:\n",
    "Yugoslavia; Hungary\n",
    "\n",
    "### Paragraph:\n",
    "Cuba and the United States were the adversaries in a crisis from 15 to 24 April 1961. A series of incursions into Caribbean and Central American states, in which Cuba was perceived to be involved, took place in 1959-60 In addition, the U.S. had become increasingly concerned about the strong ties that had developed between Cuba and the USSR. Relations between the U.S. and Cuba were strained; and the latter appealed to the Security Council at the end of 1960 charging the U.S. with plans to mount an invasion of Cuba using Cuban exiles trained in Guatemala. Diplomatic relations between the two countries were broken on 3 January 1961. Cuba and the United States were the adversaries in a crisis from 15 to 24 April 1961. A series of incursions into Caribbean and Central American states, in which Cuba was perceived to be involved, took place in 1959-60 In addition, the U.S. had become increasingly concerned about the strong ties that had developed between Cuba and the USSR. Relations between the U.S. and Cuba were strained; and the latter appealed to the Security Council at the end of 1960 charging the U.S. with plans to mount an invasion of Cuba using Cuban exiles trained in Guatemala. Diplomatic relations between the two countries were broken on 3 January 1961.\n",
    "\n",
    "### Final Sentence:\n",
    "In early April 1961 President Kennedy authorized an invasion of Cuba by the exiles, reasoning that, in the event of a failure, the remaining force would be able to establish guerrilla bases in the Cuban mountains.\n",
    "\n",
    "### Thought: \n",
    "The entities mentioned in that last sentence are \"President Kennedy\", \"Cuba\", \"the exiles\", and \"the Cuban mountains\".\n",
    "\"President Kennedy\" refers to President John F. Kennedy who is leader who represents the United States which is a country and so United States should be included.\n",
    "\"Cuba\" is a country and so Cuba should be included.\n",
    "\"The exiles\" refers to Cuban Exiles which is independent group and not a country or representing a country and so should be excluded.\n",
    "\"The Cuban mountains\" refers to a geographic feature that is part of Cuba and so Cuba should be included.\n",
    "\n",
    "### Final Answer:\n",
    "United States; Cuba\n",
    "\n",
    "### Paragraph:\"\"\"\n",
    "\n",
    "this_obs = \"\"\"\n",
    "India experienced a crisis over Hyderabad from 21 August to 18 September 1948. Hyderabad was the second largest princely state in the subcontinent, after Jammu and Kashmir. It is located in the geographic center of India and, in 1948, 80 percent of its population was Hindu while most of the political elite were Muslim. The nizam of Hyderabad did not favor accession to distant Pakistan.\n",
    "\n",
    "### Final Sentence:\n",
    "Nor did predominantly Hindu India attract him.\n",
    "\"\"\"\n",
    "prompt = prompt_base + this_obs\n",
    "prompt_model = f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "input_ids = tokenizer(prompt_model, return_tensors=\"pt\").input_ids.cuda()\n",
    "#print(input_ids.shape) #Ok if we pass too long a prompt it goes OOM. We have to catch that. #2048\n",
    "assert(input_ids.shape[0]+250 < 2048 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a semicolon separated list of countries and coalitions of countries mentioned in final sentence of the paragraph. If necessary, first perform coreference resolution and map ambiguous pronouns or references to named entities mentioned earlier in the paragraph. If an entity such as a leader or a military unit represents a country then include the country that entity represents instead of the entity. Explain your reasoning and then give a final answer.\n",
      "\n",
      "### Paragraph:\n",
      "The Communist regime, which had attained power in Russia on 7 November 1917, opted to withdraw from World War I through a separate peace with Germany--the Treaty of Brest-Litovsk on 3 March 1918.\n",
      "\n",
      "### Final Sentence:\n",
      "The Western Allies were anxious to maintain an eastern front against Germany and wished to prevent the Germans from seizing large stocks of arms in Vladivostok.\n",
      "\n",
      "### Thought:\n",
      "The entities mentioned in the last sentence are \"Western Allies\", \"Germany\", and \"Vladivostok\".\n",
      "\"Western Allies\" refers to a coalition of countries who are not explicitly listed yet in the paragraph and so Western Allies should be included.\n",
      "\"Germany\" refers to a country and so Germany should be included.\n",
      "\"Vladivostok\" refers to a city in Russia and so Russia should be included.\n",
      "\n",
      "### Final Answer:\n",
      "Western Allies; Germany; Russia\n",
      "\n",
      "### Paragraph:\n",
      "Three of the Austro-Hungarian Empire's successor states--Czechoslovakia, Yugoslavia, and Hungary--experienced a two-phase crisis from 27 March until 15 November 1921. Karl IV, the last Hapsburg Emperor of Austria-Hungary, abdicated in November 1918 and was exiled to Switzerland; but he did not abdicate his right to the throne. Early in 1921 the Conference of Ambassadors, acting for the Supreme Council (Big Ten) at the Paris Peace Conference, issued a decree forbidding the restoration of the Hapsburg family to the vacant Hungarian throne. Moreover, several European governments issued statements indicating what recourse they might have if a Hapsburg reascended the throne in Budapest. On 27 March 1921 Karl unexpectedly returned to his homeland and called upon the Regent of Hungary, Admiral Horthy, to transfer his powers to the rightful ruler. This act triggered a crisis for Czechoslovakia and Yugoslavia.\n",
      "\n",
      "### Final Sentence:\n",
      "The latter responded two days later by declaring that it would regard any attempt to restore Karl as a casus belli.\n",
      "\n",
      "### Thought:\n",
      "The entities mentioned in the last sentence are \"The latter\", an implied actor who might \"attempt to restore Karl\", and \"Karl\".\n",
      "\"The latter' refers to Yugoslavia from the previous sentence which is a country and so Yugoslavia should be included.\n",
      "\"Any attempt to restore Karl\" refers to Hungary which is a country and so Hungary should be included.\n",
      "\n",
      "### Final Answer:\n",
      "Yugoslavia; Hungary\n",
      "\n",
      "### Paragraph:\n",
      "Cuba and the United States were the adversaries in a crisis from 15 to 24 April 1961. A series of incursions into Caribbean and Central American states, in which Cuba was perceived to be involved, took place in 1959-60 In addition, the U.S. had become increasingly concerned about the strong ties that had developed between Cuba and the USSR. Relations between the U.S. and Cuba were strained; and the latter appealed to the Security Council at the end of 1960 charging the U.S. with plans to mount an invasion of Cuba using Cuban exiles trained in Guatemala. Diplomatic relations between the two countries were broken on 3 January 1961. Cuba and the United States were the adversaries in a crisis from 15 to 24 April 1961. A series of incursions into Caribbean and Central American states, in which Cuba was perceived to be involved, took place in 1959-60 In addition, the U.S. had become increasingly concerned about the strong ties that had developed between Cuba and the USSR. Relations between the U.S. and Cuba were strained; and the latter appealed to the Security Council at the end of 1960 charging the U.S. with plans to mount an invasion of Cuba using Cuban exiles trained in Guatemala. Diplomatic relations between the two countries were broken on 3 January 1961.\n",
      "\n",
      "### Final Sentence:\n",
      "In early April 1961 President Kennedy authorized an invasion of Cuba by the exiles, reasoning that, in the event of a failure, the remaining force would be able to establish guerrilla bases in the Cuban mountains.\n",
      "\n",
      "### Thought: \n",
      "The entities mentioned in that last sentence are \"President Kennedy\", \"Cuba\", \"the exiles\", and \"the Cuban mountains\".\n",
      "\"President Kennedy\" refers to President John F. Kennedy who is leader who represents the United States which is a country and so United States should be included.\n",
      "\"Cuba\" is a country and so Cuba should be included.\n",
      "\"The exiles\" refers to Cuban Exiles which is independent group and not a country or representing a country and so should be excluded.\n",
      "\"The Cuban mountains\" refers to a geographic feature that is part of Cuba and so Cuba should be included.\n",
      "\n",
      "### Final Answer:\n",
      "United States; Cuba\n",
      "\n",
      "### Paragraph:\n",
      "India experienced a crisis over Hyderabad from 21 August to 18 September 1948. Hyderabad was the second largest princely state in the subcontinent, after Jammu and Kashmir. It is located in the geographic center of India and, in 1948, 80 percent of its population was Hindu while most of the political elite were Muslim. The nizam of Hyderabad did not favor accession to distant Pakistan.\n",
      "\n",
      "### Final Sentence:\n",
      "Nor did predominantly Hindu India attract him.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens=250\n",
    "temperature=0.7\n",
    "output = model.generate(inputs=input_ids, temperature=temperature, max_new_tokens=max_new_tokens) #uh oh, 1966 was too big too. Do both the max tokens and prompt have to fit in the context window?\n",
    "answer = tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The entities mentioned in the last sentence are \"Nizam of Hyderabad\" and \"India\".\n",
      "\"Nizam of Hyderabad\" refers to a leader who represents Hyderabad and so Hyderabad should be included.\n",
      "\"India\" is a country and so India should be included.\n",
      "\n",
      "### Final Answer:\n",
      "Hyderabad; India</s>\n"
     ]
    }
   ],
   "source": [
    "print(answer.replace(prompt,'').replace('<s>### Human: \\n','').replace('<s> ### Human: ','').replace('\\n### Assistant:',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_name = \"yhat_countries\"\n",
    "import pandas as pd\n",
    "import os\n",
    "max_new_tokens=250\n",
    "temperature=0.7\n",
    "\n",
    "ICBeLLM_xy = pd.read_csv(\"/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/ICBeLLM_xy.tsv\", sep=\"\\t\", na_values='').reset_index(drop=True).fillna('')\n",
    "ICBeLLM_xy.story_so_far = ICBeLLM_xy.story_so_far.fillna('')\n",
    "file=\"/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/\" + variable_name + \".tsv\"\n",
    "if os.path.exists(file):\n",
    "    temp=pd.read_csv(file,sep=\"\\t\", na_values='').fillna('')\n",
    "else:\n",
    "    temp=ICBeLLM_xy.copy()\n",
    "    temp.loc[:,'model']=quantized_model_dir\n",
    "    temp.loc[:,'max_new_tokens']=max_new_tokens\n",
    "    temp.loc[:,'temperature']=temperature\n",
    "    temp.loc[:,'variable']=variable_name\n",
    "    temp.loc[:,'time'] = None\n",
    "    temp.loc[:,'n_tokens'] = None\n",
    "    temp.loc[:,'prompt']=None\n",
    "    temp.loc[:,'response']=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539.0\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens=250\n",
    "i=1\n",
    "prompt = prompt_base + temp['story_so_far'].values[i] + \"\\n\\n### Final Sentence:\\n\" + \"\\n\"\n",
    "prompt_model = f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "tokens_remaining = 2048 - max_new_tokens - len(prompt_model)/4\n",
    "print(tokens_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "characters_remaining = np.round((tokens_remaining*4))\n",
    "#print(characters_remaining)\n",
    "n=len(temp['sentence_span_text'].values[i])\n",
    "#print(n)\n",
    "start=max(n-characters_remaining, 0)\n",
    "temp['sentence_span_text'].values[i][start:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "195-60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = prompt_base + temp['story_so_far'].values[i] + \"\\n\\n### Final Sentence:\\n\" + temp['sentence_span_text'].values[i][-(tokens_remaining*4):] + \"\\n\"\n",
    "#prompt_model = f'''### Human: {prompt}\n",
    "#### Assistant:'''\n",
    "#\n",
    "#print(prompt_model)\n",
    "#input_ids = tokenizer(prompt_model, return_tensors=\"pt\").input_ids.cuda()\n",
    "#print(input_ids.shape) #Ok if we pass too long a prompt it goes OOM. We have to catch that. #2048\n",
    "#assert(input_ids.shape[0]+250 < 2048 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2358\n",
      "861\n",
      "1982\n"
     ]
    }
   ],
   "source": [
    "i=618\n",
    "int(i)\n",
    "prompt = prompt_base + temp['sentence_span_text'].values[i] + \"\\n\\n### Final Sentence:\\n\" + \"\\n\"\n",
    "prompt_model = f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "tokens_remaining = 2048 - max_new_tokens - len(prompt_model)/4 - 150 #750 #150 #take off another buffer 150 characters just to be safe\n",
    "characters_remaining = np.round((tokens_remaining*4))\n",
    "n=len(temp['story_so_far'].values[i])\n",
    "print(n)\n",
    "start= int(max(n-characters_remaining, 0))\n",
    "print(start)\n",
    "prompt = prompt_base + \"\\n\" + temp['story_so_far'].values[i][start:n] + \"\\n\\n### Final Sentence:\\n\" + temp['sentence_span_text'].values[i] + \"\\n\"\n",
    "prompt_model = f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "input_ids = tokenizer(prompt_model, return_tensors=\"pt\").input_ids.cuda()\n",
    "print(input_ids.shape[1]+max_new_tokens)\n",
    "assert(input_ids.shape[1]+max_new_tokens < 2048 ) #I'm getting OOM errors at least at 1961, 1629 is ok\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=max_new_tokens) #uh oh, 1966 was too big too. Do both the max tokens and prompt have to fit in the context window?\n",
    "answer = tokenizer.decode(output[0])\n",
    "temp.loc[i,'prompt'] = prompt_model\n",
    "temp.loc[i,'response'] = answer.replace(prompt_model,'')\n",
    "\n",
    "#temp.to_csv(file, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1982"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_ids.shape[1] + max_new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[21], line 21\u001b[0m\n",
      "\u001b[1;32m     19\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(prompt_model, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mcuda()\n",
      "\u001b[1;32m     20\u001b[0m \u001b[39massert\u001b[39;00m(input_ids\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39mmax_new_tokens \u001b[39m<\u001b[39m \u001b[39m2048\u001b[39m )\n",
      "\u001b[0;32m---> 21\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(inputs\u001b[39m=\u001b[39;49minput_ids, temperature\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m, max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens) \u001b[39m#uh oh, 1966 was too big too. Do both the max tokens and prompt have to fit in the context window?\u001b[39;00m\n",
      "\u001b[1;32m     22\u001b[0m answer \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m])\n",
      "\u001b[1;32m     23\u001b[0m stopt_time \u001b[39m=\u001b[39m timeit\u001b[39m.\u001b[39mdefault_timer() \n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/auto_gptq/modeling/_base.py:423\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    421\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"shortcut for model.generate\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    422\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode(), torch\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(device_type\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype):\n",
      "\u001b[0;32m--> 423\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n",
      "\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n",
      "\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1515\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1509\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n",
      "\u001b[1;32m   1510\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1 when doing greedy search, \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m   1511\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut is \u001b[39m\u001b[39m{\u001b[39;00mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m   1512\u001b[0m         )\n",
      "\u001b[1;32m   1514\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n",
      "\u001b[0;32m-> 1515\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n",
      "\u001b[1;32m   1516\u001b[0m         input_ids,\n",
      "\u001b[1;32m   1517\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n",
      "\u001b[1;32m   1518\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n",
      "\u001b[1;32m   1519\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n",
      "\u001b[1;32m   1520\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n",
      "\u001b[1;32m   1521\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n",
      "\u001b[1;32m   1522\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n",
      "\u001b[1;32m   1523\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n",
      "\u001b[1;32m   1524\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n",
      "\u001b[1;32m   1525\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n",
      "\u001b[1;32m   1526\u001b[0m     )\n",
      "\u001b[1;32m   1528\u001b[0m \u001b[39melif\u001b[39;00m is_contrastive_search_gen_mode:\n",
      "\u001b[1;32m   1529\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/generation/utils.py:2332\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n",
      "\u001b[1;32m   2329\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n",
      "\u001b[1;32m   2331\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n",
      "\u001b[0;32m-> 2332\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n",
      "\u001b[1;32m   2333\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n",
      "\u001b[1;32m   2334\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n",
      "\u001b[1;32m   2335\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n",
      "\u001b[1;32m   2336\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n",
      "\u001b[1;32m   2337\u001b[0m )\n",
      "\u001b[1;32m   2339\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n",
      "\u001b[1;32m   2340\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n",
      "\u001b[1;32m    685\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n",
      "\u001b[1;32m    687\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n",
      "\u001b[0;32m--> 688\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n",
      "\u001b[1;32m    689\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n",
      "\u001b[1;32m    690\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n",
      "\u001b[1;32m    691\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n",
      "\u001b[1;32m    692\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n",
      "\u001b[1;32m    693\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n",
      "\u001b[1;32m    694\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n",
      "\u001b[1;32m    695\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n",
      "\u001b[1;32m    696\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n",
      "\u001b[1;32m    697\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n",
      "\u001b[1;32m    698\u001b[0m )\n",
      "\u001b[1;32m    700\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;32m    701\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:578\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n",
      "\u001b[1;32m    570\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n",
      "\u001b[1;32m    571\u001b[0m         create_custom_forward(decoder_layer),\n",
      "\u001b[1;32m    572\u001b[0m         hidden_states,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    575\u001b[0m         \u001b[39mNone\u001b[39;00m,\n",
      "\u001b[1;32m    576\u001b[0m     )\n",
      "\u001b[1;32m    577\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 578\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n",
      "\u001b[1;32m    579\u001b[0m         hidden_states,\n",
      "\u001b[1;32m    580\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n",
      "\u001b[1;32m    581\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n",
      "\u001b[1;32m    582\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n",
      "\u001b[1;32m    583\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n",
      "\u001b[1;32m    584\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n",
      "\u001b[1;32m    585\u001b[0m     )\n",
      "\u001b[1;32m    587\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;32m    589\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:293\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n",
      "\u001b[1;32m    290\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n",
      "\u001b[1;32m    292\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n",
      "\u001b[0;32m--> 293\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n",
      "\u001b[1;32m    294\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n",
      "\u001b[1;32m    295\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n",
      "\u001b[1;32m    296\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n",
      "\u001b[1;32m    297\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n",
      "\u001b[1;32m    298\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n",
      "\u001b[1;32m    299\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n",
      "\u001b[1;32m    300\u001b[0m )\n",
      "\u001b[1;32m    301\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "\u001b[1;32m    303\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/auto_gptq/nn_modules/fused_llama_attn.py:63\u001b[0m, in \u001b[0;36mFusedLlamaAttentionForQuantizedModel.forward\u001b[0;34m(self, hidden_states, past_key_value, attention_mask, position_ids, output_attentions, use_cache, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m     62\u001b[0m     kv_seq_len \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m past_key_value[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n",
      "\u001b[0;32m---> 63\u001b[0m cos, sin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrotary_emb(value_states, seq_len\u001b[39m=\u001b[39;49mkv_seq_len)\n",
      "\u001b[1;32m     64\u001b[0m query_states, key_states \u001b[39m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
      "\u001b[1;32m     65\u001b[0m \u001b[39m# [bsz, nh, t, hd]\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:121\u001b[0m, in \u001b[0;36mLlamaRotaryEmbedding.forward\u001b[0;34m(self, x, seq_len)\u001b[0m\n",
      "\u001b[1;32m    118\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\u001b[39m\"\u001b[39m\u001b[39mcos_cached\u001b[39m\u001b[39m\"\u001b[39m, emb\u001b[39m.\u001b[39mcos()[\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, :, :], persistent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m    119\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\u001b[39m\"\u001b[39m\u001b[39msin_cached\u001b[39m\u001b[39m\"\u001b[39m, emb\u001b[39m.\u001b[39msin()[\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, :, :], persistent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m    120\u001b[0m \u001b[39mreturn\u001b[39;00m (\n",
      "\u001b[0;32m--> 121\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcos_cached[:, :, :seq_len, \u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m]\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39;49mx\u001b[39m.\u001b[39;49mdtype),\n",
      "\u001b[1;32m    122\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msin_cached[:, :, :seq_len, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdtype),\n",
      "\u001b[1;32m    123\u001b[0m )\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "#65 fails\n",
    "#Grab the last 2k characters of the story to avoid oom\n",
    "for i in range(temp.shape[0]):\n",
    "    if pd.isna(temp['response'][i]) or temp['response'][i]=='' :\n",
    "        start_time = timeit.default_timer()\n",
    "        #print(i)\n",
    "        prompt = prompt_base + temp['sentence_span_text'].values[i] + \"\\n\\n### Final Sentence:\\n\" + \"\\n\"\n",
    "        prompt_model = f'''### Human: {prompt}\n",
    "        ### Assistant:'''\n",
    "        tokens_remaining = 2048 - max_new_tokens - len(prompt_model)/4 - 150 #take off 150 just to be safe\n",
    "        characters_remaining = np.round((tokens_remaining*4))\n",
    "        n=len(temp['story_so_far'].values[i])\n",
    "        start= int(max(n-characters_remaining, 0))\n",
    "        prompt = prompt_base + \"\\n\" + temp['story_so_far'].values[i][start:n] + \"\\n\\n### Final Sentence:\\n\" + temp['sentence_span_text'].values[i] + \"\\n\"\n",
    "        prompt_model = f'''### Human: {prompt}\n",
    "        ### Assistant:'''\n",
    "        input_ids = tokenizer(prompt_model, return_tensors=\"pt\").input_ids.cuda()\n",
    "        assert(input_ids.shape[1]+max_new_tokens < 2048 )\n",
    "        output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=max_new_tokens) #uh oh, 1966 was too big too. Do both the max tokens and prompt have to fit in the context window?\n",
    "        answer = tokenizer.decode(output[0])\n",
    "        stopt_time = timeit.default_timer() \n",
    "        temp.loc[i,'time'] = round(stopt_time- start_time,3)\n",
    "        temp.loc[i,'n_tokens']=input_ids.shape[1]\n",
    "        temp.loc[i,'prompt'] = prompt_model\n",
    "        temp.loc[i,'response'] = answer.replace(prompt_model,'')\n",
    "        \n",
    "        temp.to_csv(file, sep=\"\\t\", index=False)\n",
    "        #torch.cuda.empty_cache()\n",
    "\n",
    "#ICBeLLM_xy['prompt_entity_list_explicit']\n",
    "#[prompt]\n",
    "#ICBeLLM_xy['story_so_far'].fillna('').values[i] \n",
    "#pd.isna(temp['yhat_prompt_entity_yes_no'][1])\n",
    "#temp['yhat_prompt_entity_yes_no'][0] = \"asdasdasd\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "718"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i #618\n",
    "#temp['story_so_far'].values[i]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
