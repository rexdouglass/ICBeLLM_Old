{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ/discussions/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece\n",
    "#!pip install tokenizers -U\n",
    "#!pip install transformers[sentencepiece] -U\n",
    "#!pip install accelerate -U\n",
    "#!pip install optimum\n",
    "\n",
    "#https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ/discussions/5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/skynet3/8tb_a/rwd_github_private/ICBeLLM'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "cwd\n",
    "os.chdir('/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/')\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3090 Ti'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" #auto gets confused for some reason and puts it on the 2080\n",
    "import torch\n",
    "torch.cuda.device_count()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skynet3/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "\n",
    "# If you've already downloaded the model, reference its location here:\n",
    "#quantized_model_dir = \"/path/to/TheBloke_WizardLM-Uncensored-Falcon-40B-3bit-GPTQ\"\n",
    "# Or to download it from the hub and store it in the Hugging Face cache directory:\n",
    "\n",
    "#quantized_model_dir = \"./data_temp/Wizard-Vicuna-13B-Uncensored-GPTQ/\" #TheBloke_WizardLM-Uncensored-Falcon-40B-3bit-GPTQ\n",
    "#quantized_model_dir = \"TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ\"\n",
    "quantized_model_dir = \"TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ\"\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The safetensors archive passed at /home/skynet3/.cache/huggingface/hub/models--TheBloke--Wizard-Vicuna-30B-Uncensored-GPTQ/snapshots/7aeb8db6d49732afa76dc7ce60c5c040dbbcf3ea/Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    }
   ],
   "source": [
    "#FileNotFoundError: Could not find model in TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ\n",
    "#https://huggingface.co/TheBloke/wizard-vicuna-13B-GPTQ/discussions/7\n",
    "#quantized_model_dir = \"TheBloke/stable-vicuna-13B-GPTQ\"\n",
    "#model_basename = \"wizard-vicuna-13B-GPTQ-4bit.compat.no-act-order\" #YOU HAVE TO DO THIS\n",
    "\n",
    "#model_basename = \"Wizard-vicuna-30B-Uncensored-GPTQ-4bit.act-order\"\n",
    "# path to directory containing local model\n",
    "#https://huggingface.co/TheBloke/starcoderplus-GPTQ/discussions/3\n",
    "quantized_model_dir = \"TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ\" #\"/workspace/models/TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ\"\n",
    "model_basename = \"Wizard-Vicuna-30B-Uncensored-GPTQ-4bit.act.order\" #.safetensors\n",
    "use_triton = False \n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, \n",
    "                                           device=\"cuda:0\", \n",
    "                                           use_triton=False,\n",
    "                                           use_safetensors=True , \n",
    "                                           model_basename=model_basename,\n",
    "                                           quantize_config=None,\n",
    "                                           #torch_dtype=torch.bfloat16, \n",
    "                                           trust_remote_code=True\n",
    "                                           ) #\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hf_device_map #yup, it offloaded the last layers to the CPU. That's why it's so slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Output ***\n",
      "<s> ### Human: Ask Something here\n",
      "### Assistant: Sure, I can help you with anything related to the topic of this book. What would you like to know?</s>\n",
      "**************\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Ask Something here\"\n",
    "prompt_model = f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "input_ids = tokenizer(prompt_model, return_tensors=\"pt\").input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "\n",
    "print(\"*** Output ***\")\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(\"**************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok it continues to be about 4 characters per token\n",
    "# print(input_ids.shape) #So this prompt and a little bit of story is about 939 tokens, got another 1k to play with\n",
    "#print(len(prompt))\n",
    "#5046/1287\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Create a semicolon separated list of countries and coalitions of countries mentioned in final sentence of the paragraph. If necessary, first perform coreference resolution and map ambigious pronouns or rerences to named entities mentioned earlier in the paragraph. If an entity such as a leader or a military unit represents a country then include the country that entity represents instead of the entity. Explain your reasoning and then give a final answer.\n",
    "\n",
    "### Paragraph:\n",
    "The Communist regime, which had attained power in Russia on 7 November 1917, opted to withdraw from World War I through a separate peace with Germany--the Treaty of Brest-Litovsk on 3 March 1918.\n",
    "\n",
    "### Final Sentence:\n",
    "The Western Allies were anxious to maintain an eastern front against Germany and wished to prevent the Germans from seizing large stocks of arms in Vladivostok.\n",
    "\n",
    "### Thought:\n",
    "The entities mentioned in the last sentence are \"Western Allies\", \"Germany\", and \"Vladivostok\".\n",
    "\"Western Allies\" refers to a coalition of countries who are not explicitly listed yet in the paragraph and so Western Allies should be included.\n",
    "\"Germany\" refers to a country and so Germany should be included.\n",
    "\"Vladivostok\" refers to a city in Russia and so Russia should be included.\n",
    "\n",
    "### Final Answer:\n",
    "Western Allies; Germany; Russia\n",
    "\n",
    "### Paragraph:\n",
    "Three of the Austro-Hungarian Empire's successor states--Czechoslovakia, Yugoslavia, and Hungary--experienced a two-phase crisis from 27 March until 15 November 1921. Karl IV, the last Hapsburg Emperor of Austria-Hungary, abdicated in November 1918 and was exiled to Switzerland; but he did not abdicate his right to the throne. Early in 1921 the Conference of Ambassadors, acting for the Supreme Council (Big Ten) at the Paris Peace Conference, issued a decree forbidding the restoration of the Hapsburg family to the vacant Hungarian throne. Moreover, several European governments issued statements indicating what recourse they might have if a Hapsburg reascended the throne in Budapest. On 27 March 1921 Karl unexpectedly returned to his homeland and called upon the Regent of Hungary, Admiral Horthy, to transfer his powers to the rightful ruler. This act triggered a crisis for Czechoslovakia and Yugoslavia.\n",
    "\n",
    "### Final Sentence:\n",
    "The latter responded two days later by declaring that it would regard any attempt to restore Karl as a casus belli.\n",
    "\n",
    "### Thought:\n",
    "The entities mentioned in the last sentence are \"The latter\", an implied actor who might \"attempt to restore Karl\", and \"Karl\".\n",
    "\"The latter' refers to Yugoslavia from the previous sentence which is a country and so Yugoslavia should be included.\n",
    "\"Any attempt to restore Karl\" refers to Hungary which is a country and so Hungary should be included.\n",
    "\n",
    "### Final Answer:\n",
    "Yugoslavia; Hungary\n",
    "\n",
    "### Paragraph:\n",
    "Cuba and the United States were the adversaries in a crisis from 15 to 24 April 1961. A series of incursions into Caribbean and Central American states, in which Cuba was perceived to be involved, took place in 1959-60 In addition, the U.S. had become increasingly concerned about the strong ties that had developed between Cuba and the USSR. Relations between the U.S. and Cuba were strained; and the latter appealed to the Security Council at the end of 1960 charging the U.S. with plans to mount an invasion of Cuba using Cuban exiles trained in Guatemala. Diplomatic relations between the two countries were broken on 3 January 1961. Cuba and the United States were the adversaries in a crisis from 15 to 24 April 1961. A series of incursions into Caribbean and Central American states, in which Cuba was perceived to be involved, took place in 1959-60 In addition, the U.S. had become increasingly concerned about the strong ties that had developed between Cuba and the USSR. Relations between the U.S. and Cuba were strained; and the latter appealed to the Security Council at the end of 1960 charging the U.S. with plans to mount an invasion of Cuba using Cuban exiles trained in Guatemala. Diplomatic relations between the two countries were broken on 3 January 1961.\n",
    "\n",
    "### Final Sentence:\n",
    "In early April 1961 President Kennedy authorized an invasion of Cuba by the exiles, reasoning that, in the event of a failure, the remaining force would be able to establish guerrilla bases in the Cuban mountains.\n",
    "\n",
    "### Thought: \n",
    "The entities mentioned in that last sentence are \"President Kennedy\", \"Cuba\", \"the exiles\", and \"the Cuban mountains\".\n",
    "\"President Kennedy\" refers to President John F. Kennedy who is leader who represents the United States which is a country and so United States should be included.\n",
    "\"Cuba\" is a country and so Cuba should be included.\n",
    "\"The exiles\" refers to Cuban Exiles which is independent group and not a country or representing a country and so should be excluded.\n",
    "\"The Cuban mountains\" refers to a geographic feature that is part of Cuba and so Cuba should be included.\n",
    "\n",
    "### Final Answer:\n",
    "United States; Cuba\"\n",
    "\n",
    "### Paragraph:\n",
    "India experienced a crisis over Hyderabad from 21 August to 18 September 1948. Hyderabad was the second largest princely state in the subcontinent, after Jammu and Kashmir. It is located in the geographic center of India and, in 1948, 80 percent of its population was Hindu while most of the political elite were Muslim. The nizam of Hyderabad did not favor accession to distant Pakistan.\n",
    "\n",
    "### Final Sentence:\n",
    "Nor did predominantly Hindu India attract him.\n",
    "\"\"\"\n",
    "prompt_model = f'''### Human: {prompt}\n",
    "### Assistant:'''\n",
    "input_ids = tokenizer(prompt_model, return_tensors=\"pt\").input_ids.cuda()\n",
    "#print(input_ids.shape) #Ok if we pass too long a prompt it goes OOM. We have to catch that. #2048\n",
    "assert(input_ids.shape[0]+250 < 2048 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=250) #uh oh, 1966 was too big too. Do both the max tokens and prompt have to fit in the context window?\n",
    "\n",
    "answer = tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer.replace(prompt,'').replace('<s>### Human: \\n',''))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "ICBeLLM_xy = pd.read_csv(\"/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/ICBeLLM_xy.tsv\", sep=\"\\t\")\n",
    "\n",
    "ICBeLLM_xy.columns\n",
    "import os\n",
    "file=\"/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/yhat_prompt_entity_yes_no.tsv\"\n",
    "if os.path.exists(file):\n",
    "    temp=pd.read_csv(file,sep=\"\\t\")\n",
    "else:\n",
    "    temp=ICBeLLM_xy[['crisno', 'sentence_number_int_aligned', 'crisis_text','sentence_span_text']].copy()\n",
    "    temp.loc[:,'yhat_prompt_entity_yes_no']=None\n",
    "    temp.loc[:,'yhat_prompt_entity_list_explicit']=None\n",
    "    \n",
    "#65 fails\n",
    "#Grab the last 2k characters of the story to avoid oom\n",
    "for i in range(ICBeLLM_xy.shape[0]):\n",
    "    if pd.isna(temp['yhat_prompt_entity_yes_no'][i]):\n",
    "        #prompt = \"Story: \" + ICBeLLM_xy['story_so_far'].fillna('').values[i][-2000:] + \"\\n Task: \" + ICBeLLM_xy['prompt_entity_yes_no'].values[i] + \"\\nSentence: \" + ICBeLLM_xy['sentence_span_text'].values[i] + \"\\n\\n### Response:\"\n",
    "        prompt = ICBeLLM_xy['prompt_entity_yes_no'].values[i] + \"\\n\\n\" + ICBeLLM_xy['sentence_span_text'].values[i] + \"\\n\\nAnswer:\"\n",
    "        #print(prompt)\n",
    "        tokens = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\").input_ids\n",
    "        output = model.generate(input_ids=tokens, max_new_tokens=1, do_sample=True, temperature=0.8)\n",
    "        result=tokenizer.decode(output[0]).replace(prompt,'')\n",
    "        #print(result)\n",
    "        temp.loc[i,'yhat_prompt_entity_yes_no'] = result\n",
    "        #print()\n",
    "\n",
    "    if pd.isna(temp['yhat_prompt_entity_list_explicit'][i]):\n",
    "        #prompt = \"Story: \" + ICBeLLM_xy['story_so_far'].fillna('').values[i][-2000:] + \"\\n Task: \" + ICBeLLM_xy['prompt_entity_list_explicit'].values[i] + \"\\nSentence: \" + ICBeLLM_xy['sentence_span_text'].values[i] + \"\\n\\n### Response:\"\n",
    "        prompt = ICBeLLM_xy['prompt_entity_list_explicit'].values[i] + \"\\n\\n\" + ICBeLLM_xy['sentence_span_text'].values[i] + \"\\n\\nCountries and Organizations:\"\n",
    "        #print(prompt)\n",
    "        tokens = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\").input_ids\n",
    "        output = model.generate(input_ids=tokens, max_new_tokens=20, do_sample=True, temperature=0.8)\n",
    "        result=tokenizer.decode(output[0]).replace(prompt,'')\n",
    "        #print(result)\n",
    "        temp.loc[i,'yhat_prompt_entity_list_explicit'] = result\n",
    "        \n",
    "    temp.to_csv(\"/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/yhat_prompt_entity_yes_no.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "  ICBeLLM_xy['prompt_entity_list_explicit']\n",
    "[prompt]\n",
    "ICBeLLM_xy['story_so_far'].fillna('').values[i] \n",
    "pd.isna(temp['yhat_prompt_entity_yes_no'][1])\n",
    "\n",
    "\n",
    "temp['yhat_prompt_entity_yes_no'][0] = \"asdasdasd\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
