{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skynet3/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "The safetensors archive passed at /home/skynet3/.cache/huggingface/hub/models--TheBloke--WizardLM-Uncensored-Falcon-40B-3bit-GPTQ/snapshots/5c2c5025f2bcbd13f76b988ce29361aac92efb43/gptq_model-3bit--1g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "can't get model's sequence length from model config, will set to 4096.\n",
      "RWGPTQForCausalLM hasn't fused attention module yet, will skip inject fused attention.\n",
      "RWGPTQForCausalLM hasn't fused mlp module yet, will skip inject fused mlp.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "\n",
    "# If you've already downloaded the model, reference its location here:\n",
    "#quantized_model_dir = \"/path/to/TheBloke_WizardLM-Uncensored-Falcon-40B-3bit-GPTQ\"\n",
    "# Or to download it from the hub and store it in the Hugging Face cache directory:\n",
    "quantized_model_dir = \"TheBloke/WizardLM-Uncensored-Falcon-40B-3bit-GPTQ\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=False)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device=\"cuda:0\", use_triton=False, use_safetensors=True, torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "\n",
    "prompt = \"What is a falcon? Can I keep one as a pet?\"\n",
    "prompt_template = f\"{prompt}\\n### Response:\"\n",
    "\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda:0\").input_ids\n",
    "output = model.generate(input_ids=tokens, max_new_tokens=100, do_sample=True, temperature=0.8)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a falcon? Can I keep one as a pet?\n",
      "### Response:A falcon is a type of bird of prey, characterized by their strong, sharp beaks and powerful talons. They are known for their exceptional hunting abilities and are often used in falconry, a type of hunting where trained falcons are used to catch game.\n",
      "While it is possible to keep a falcon as a pet, it is not recommended. Falcons are highly specialized birds that require specialized care and training. They are not domesticated animals and require a great deal of attention and\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the sentence describe an action or an interaction? Action describes an action, not an interaction, meaning that only one actor is taking an action. Interaction occurs in tandem with another actor and/or involve another actor.\n",
      "They then signed a different treaty in 1980.\n",
      "\n",
      "### Response:This sentence describes an interaction as it involves two actors (They and the other party who signed the treaty in 1980).<|endoftext|>The first part\n"
     ]
    }
   ],
   "source": [
    "coding_instructions = \"Does the sentence describe an action or an interaction? Action describes an action, not an interaction, meaning that only one actor is taking an action. Interaction occurs in tandem with another actor and/or involve another actor.\"\n",
    "sentence=\"They then signed a different treaty in 1980.\"\n",
    "prompt_template=coding_instructions + \"\\n\" + sentence + \"\\n\\n### Response:\"\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda:0\").input_ids\n",
    "output = model.generate(input_ids=tokens, max_new_tokens=30, do_sample=True, temperature=0.8)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the sentence as either an action or an interaction. An action involves only one actor, and an interaction involves two or more actors.\n",
      "They then signed a different treaty in 1980.\n",
      "\n",
      "### Response:Interaction.<|endoftext|>It involves two or more actors (they).<|endoftext|>What is your name?<|endoftext|>I'm sorry, I didn't\n"
     ]
    }
   ],
   "source": [
    "coding_instructions = \"Classify the sentence as either an action or an interaction. An action involves only one actor, and an interaction involves two or more actors.\"\n",
    "sentence=\"They then signed a different treaty in 1980.\"\n",
    "prompt_template=coding_instructions + \"\\n\" + sentence + \"\\n\\n### Response:\"\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda:0\").input_ids\n",
    "output = model.generate(input_ids=tokens, max_new_tokens=30, do_sample=True, temperature=0.8)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This right now is our winner. It performs exactly like I want it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bolshevik Russia, precursor to the Soviet Union The Communist regime, which had attained power in Russia on 7 November 1917, opted to withdraw from World War I through a separate peace with Germany--the Treaty of Brest-Litovsk on 3 March 1918.\n",
      "Given the story above, classify the sentence below as either an action or an interaction. An action involves only one actor, and an interaction involves two or more actors.\n",
      "Russia fortified its border.\n",
      "\n",
      "### Response:Action.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "story= \"Bolshevik Russia, precursor to the Soviet Union The Communist regime, which had attained power in Russia on 7 November 1917, opted to withdraw from World War I through a separate peace with Germany--the Treaty of Brest-Litovsk on 3 March 1918.\"\n",
    "coding_instructions = \"Given the story above, classify the sentence below as either an action or an interaction. An action involves only one actor, and an interaction involves two or more actors.\"\n",
    "#sentence=\"They then signed a different treaty in 1980.\"\n",
    "sentence=\"Russia fortified its border.\"\n",
    "prompt_template= story + \"\\n\" + coding_instructions + \"\\n\" + sentence + \"\\n\\n### Response:\"\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda:0\").input_ids\n",
    "output = model.generate(input_ids=tokens, max_new_tokens=3, do_sample=True, temperature=0.8)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Story: There were three crisis actors in the most ominous of all East/West crises, the United States, Cuba, and the Soviet Union. The crisis lasted from 16 October to 20 November 1962. After the 1961 Bay of Pigs abortive invasion (see Case #181), Cuba became one of the central issues of U.S. foreign policy: the U.S. viewed Cuba as a potential source of communist-oriented subversive activities in Latin America. When the U.S. discovered the presence of Soviet military personnel in Cuba on 7 September 1962 it called up 150,000 reservists. The Soviets mobilized on the 11th. Although persistent rumors circulated concerning the deployment of Soviet missiles in Cuba, Soviet Ambassador Anatoly Dobrynin denied the charges, and Premier Khrushchev gave his personal assurances that ground-to-ground missiles would never be shipped to Cuba. On the eve of the Missile crisis Washington did not openly challenge the Soviet statements concerning the defensive character of the weapons being sent to Cuba. The U.S. crisis was triggered on 16 October when the CIA presented to President Kennedy photographic evidence of the presence of Soviet missiles in Cuba. The U.S. responded with a decision on the 20th to blockade all offensive military equipment en route to Cuba. When this was announced on 22 October, a crisis was triggered for Cuba and the USSR. An urgent meeting of the UN Security Council was requested by both the U.S. and Cuba on the 22nd, and by the USSR the next day.\\nTask: Given the story above, classify the sentence below as either an action or an interaction. An action involves only unilateral moves performed by actors individually, and an interaction involves bilateral moves between two actors acting jointly.\\nSentence: On the 23rd as well, the Soviets accused the United States of violating the UN Charter and announced an alert of its armed forces and those of the Warsaw Pact members.\\n\\n### Response:Interaction. This sentence involves two actors, the U.S. and the Soviet Union, acting jointly.<|endoftext|>The U.S. made a']\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"Story: There were three crisis actors in the most ominous of all East/West crises, the United States, Cuba, and the Soviet Union. The crisis lasted from 16 October to 20 November 1962. After the 1961 Bay of Pigs abortive invasion (see Case #181), Cuba became one of the central issues of U.S. foreign policy: the U.S. viewed Cuba as a potential source of communist-oriented subversive activities in Latin America. When the U.S. discovered the presence of Soviet military personnel in Cuba on 7 September 1962 it called up 150,000 reservists. The Soviets mobilized on the 11th. Although persistent rumors circulated concerning the deployment of Soviet missiles in Cuba, Soviet Ambassador Anatoly Dobrynin denied the charges, and Premier Khrushchev gave his personal assurances that ground-to-ground missiles would never be shipped to Cuba. On the eve of the Missile crisis Washington did not openly challenge the Soviet statements concerning the defensive character of the weapons being sent to Cuba. The U.S. crisis was triggered on 16 October when the CIA presented to President Kennedy photographic evidence of the presence of Soviet missiles in Cuba. The U.S. responded with a decision on the 20th to blockade all offensive military equipment en route to Cuba. When this was announced on 22 October, a crisis was triggered for Cuba and the USSR. An urgent meeting of the UN Security Council was requested by both the U.S. and Cuba on the 22nd, and by the USSR the next day.\\nTask: Given the story above, classify the sentence below as either an action or an interaction. An action involves only unilateral moves performed by actors individually, and an interaction involves bilateral moves between two actors acting jointly.\\nSentence: On the 23rd as well, the Soviets accused the United States of violating the UN Charter and announced an alert of its armed forces and those of the Warsaw Pact members.\\n\\n### Response:\"\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda:0\").input_ids\n",
    "output = model.generate(input_ids=tokens, max_new_tokens=30, do_sample=True, temperature=0.8)\n",
    "print([tokenizer.decode(output[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"Story:\\nTask: Given the story above, extract a comma separated list of every country or organization explicitly mentioned in the sentence below. \\nSentence: There were three crisis actors in the most ominous of all East/West crises, the United States, Cuba, and the Soviet Union.\\n\\n### Response:\"\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda:0\").input_ids\n",
    "output = model.generate(input_ids=tokens, max_new_tokens=15, do_sample=True, temperature=0.8)\n",
    "print(tokenizer.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story:\n",
      "Task: Given the story above, extract a comma separated list of every country or organization explicitly mentioned in the sentence below. There were three crisis actors in the most ominous of all East/West crises, the United States, Cuba, and the Soviet Union.\n",
      "Sentence: They signed a treaty later. \n",
      "\n",
      "### Response:United States, Cuba, Soviet Union.<|endoftext|>Task: Given the story above\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"Story:\\nTask: Given the story above, extract a comma separated list of every country or organization explicitly mentioned in the sentence below. There were three crisis actors in the most ominous of all East/West crises, the United States, Cuba, and the Soviet Union.\\nSentence: They signed a treaty later. \\n\\n### Response:\"\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda:0\").input_ids\n",
    "output = model.generate(input_ids=tokens, max_new_tokens=15, do_sample=True, temperature=0.8)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story: There were three crisis actors in the most ominous of all East/West crises, the United States, Cuba, and the Soviet Union.\n",
      "Task: Given the story above, extract a comma separated list of every country or organization explicitly mentioned in the sentence below.\n",
      "Sentence: The flowers were green. \n",
      "\n",
      "### Response:The sentence above does not explicitly mention any countries or organizations.<|endoftext|>Please provide\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"Story: There were three crisis actors in the most ominous of all East/West crises, the United States, Cuba, and the Soviet Union.\\nTask: Given the story above, extract a comma separated list of every country or organization explicitly mentioned in the sentence below.\\nSentence: The flowers were green. \\n\\n### Response:\"\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda:0\").input_ids\n",
    "output = model.generate(input_ids=tokens, max_new_tokens=15, do_sample=True, temperature=0.8)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story: There were three crisis actors in the most ominous of all East/West crises, the United States, Cuba, and the Soviet Union.\n",
      "Task: Given the story above, classify the sentence below as 'Yes, explicitly mentions a country or organization' or 'No, does not explicitly mention a country or organization.'\n",
      "Sentence: The flowers were green. \n",
      "\n",
      "### Response:No, does not explicitly mention a country or organization.<|endoftext|>- Yes,\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"Story: There were three crisis actors in the most ominous of all East/West crises, the United States, Cuba, and the Soviet Union.\\nTask: Given the story above, classify the sentence below as 'Yes, explicitly mentions a country or organization' or 'No, does not explicitly mention a country or organization.'\\nSentence: The flowers were green. \\n\\n### Response:\"\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda:0\").input_ids\n",
    "output = model.generate(input_ids=tokens, max_new_tokens=15, do_sample=True, temperature=0.8)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story: \n",
      "Task: Given the story above, classify the sentence below as 'Yes, explicitly mentions a country or organization' or 'No, does not explicitly mention a country or organization.'\n",
      "Sentence: There were three crisis actors in the most ominous of all East/West crises, the United States, Cuba, and the Soviet Union. \n",
      "\n",
      "### Response:Yes, explicitly mentions a country or organization.<|endoftext|>There were three crisis actors\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"Story: \\nTask: Given the story above, classify the sentence below as 'Yes, explicitly mentions a country or organization' or 'No, does not explicitly mention a country or organization.'\\nSentence: There were three crisis actors in the most ominous of all East/West crises, the United States, Cuba, and the Soviet Union. \\n\\n### Response:\"\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda:0\").input_ids\n",
    "output = model.generate(input_ids=tokens, max_new_tokens=15, do_sample=True, temperature=0.8)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"Story: There were three crisis actors in the most ominous of all East/West crises, the United States, Cuba, and the Soviet Union.\\nTask: Given the story above, identify which countries, groups, or organizations are explicitly mentioned in the sentence below. List every actor explicitly mentioned in the sentence, do not list any actors not explicitly mentioned in the sentence, and return 'None' if no actors are explicitly mentioned in the sentence. Only return the names of actors in a semicolon separated list with no other text.\\nSentence: The crisis lasted from 16 October to 20 November 1962.\\n\\n### Response:\"\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda:0\").input_ids\n",
    "output = model.generate(input_ids=tokens, max_new_tokens=10, do_sample=True, temperature=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story: There were three crisis actors in the most ominous of all East/West crises, the United States, Cuba, and the Soviet Union.\n",
      "Task: Given the story above, identify which countries, groups, or organizations are explicitly mentioned in the sentence below. List every actor explicitly mentioned in the sentence, do not list any actors not explicitly mentioned in the sentence, and return 'None' if no actors are explicitly mentioned in the sentence. Only return the names of actors in a semicolon separated list with no other text.\n",
      "Sentence: The crisis lasted from 16 October to 20 November 1962.\n",
      "\n",
      "### Response:None.<|endoftext|>Given the story above, there were\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Story: There were three crisis actors in the most ominous of all East/West crises, the United States, Cuba, and the Soviet Union.\\nTask: Given the story above, identify which countries, groups, or organizations are explicitly mentioned in the sentence below. List all actors that are explicitly mentioned in the sentence and do not list any actor not explicitly mentioned in the sentence. Only return the names of actors in a comma separated list with no other text. If no actors are mentioned, respond with only 'None.' \\nSentence: The crisis lasted from 16 October to 20 November 1962.\\n\\n### Response:None.<|endoftext|>There were no actors mentioned in the\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bolshevik Russia, precursor to the Soviet Union The Communist regime, which had attained power in Russia on 7 November 1917, opted to withdraw from World War I through a separate peace with Germany--the Treaty of Brest-Litovsk on 3 March 1918.\n",
      "Given the story above, classify the sentence below as either an action or an interaction. An action involves only one actor, and an interaction involves two or more actors.\n",
      "Russia fortified its border.\n",
      "\n",
      "### Response:interaction<|endoftext|>The sentence involves two or more actors, so it can be classified as an interaction.<|endoftext|>This means that Russia and its border are the\n"
     ]
    }
   ],
   "source": [
    "story= \"Bolshevik Russia, precursor to the Soviet Union The Communist regime, which had attained power in Russia on 7 November 1917, opted to withdraw from World War I through a separate peace with Germany--the Treaty of Brest-Litovsk on 3 March 1918.\"\n",
    "coding_instructions = \"Given the story above, classify the sentence below as either an action or an interaction. An action involves only one actor, and an interaction involves two or more actors.\"\n",
    "sentence=\"Russia fortified its border.\"\n",
    "prompt_template= story + \"\\n\" + coding_instructions + \"\\n\" + sentence + \"\\n\\n### Response:\"\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda:0\").input_ids\n",
    "output = model.generate(input_ids=tokens, max_new_tokens=30, do_sample=True, temperature=0.8)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the sentence: 'They then signed a different treaty in 1980.' from the story, and considering the codebook instructions: 'Action describes an action, not an interaction, meaning that only one actor is taking an action. Interaction occurs in tandem with another actor and/or involve another actor.' Please classify the sentence into either 'Action' or 'Interaction'.\n",
      "\n",
      "### Response:The sentence \"They then signed a different treaty in 1980\" would be classified as an Action, as it describes an action taken by one\n"
     ]
    }
   ],
   "source": [
    "coding_instructions=\"Given the sentence: 'They then signed a different treaty in 1980.' from the story, and considering the codebook instructions: 'Action describes an action, not an interaction, meaning that only one actor is taking an action. Interaction occurs in tandem with another actor and/or involve another actor.' Please classify the sentence into either 'Action' or 'Interaction'.\"\n",
    "prompt_template=coding_instructions + \"\\n\\n### Response:\"\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda:0\").input_ids\n",
    "output = model.generate(input_ids=tokens, max_new_tokens=30, do_sample=True, temperature=0.8)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The story so far: 'Bolshevik Russia, precursor to the Soviet Union The Communist regime, which had attained power in Russia on 7 November 1917, opted to withdraw from World War I through a separate peace with Germany--the Treaty of Brest-Litovsk on 3 March 1918.' \n",
      " Question: Which actors are explicitly mentioned in the next sentence of the story below? \n",
      " The next sentence: 'Russia and Germany signed a treaty.' \n",
      " Answer: The next sentence mentions Russia and Germany as the actors\n"
     ]
    }
   ],
   "source": [
    "prompt_template=\"The story so far: 'Bolshevik Russia, precursor to the Soviet Union The Communist regime, which had attained power in Russia on 7 November 1917, opted to withdraw from World War I through a separate peace with Germany--the Treaty of Brest-Litovsk on 3 March 1918.' \\n Question: Which actors are explicitly mentioned in the next sentence of the story below? \\n The next sentence: 'Russia and Germany signed a treaty.' \\n Answer:\"\n",
    "\n",
    "#prompt = \"What is a falcon? Can I keep one as a pet?\"\n",
    "#prompt_template = f\"{prompt}\\n### Response:\"\n",
    "\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda:0\").input_ids\n",
    "output = model.generate(input_ids=tokens, max_new_tokens=10, do_sample=True, temperature=0.8)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bolshevik Russia, precursor to the Soviet Union The Communist regime, which had attained power in Russia on 7 November 1917, opted to withdraw from World War I through a separate peace with Germany--the Treaty of Brest-Litovsk on 3 March 1918.\n",
      "Question: Is there a unilateral event mentioned in the next sentence of the story below?\n",
      "They then signed a different treaty in 1980.\n",
      "### Response:Yes, the unilateral event mentioned in the next sentence\n"
     ]
    }
   ],
   "source": [
    "prompt_template=\"Bolshevik Russia, precursor to the Soviet Union The Communist regime, which had attained power in Russia on 7 November 1917, opted to withdraw from World War I through a separate peace with Germany--the Treaty of Brest-Litovsk on 3 March 1918.\\nQuestion: Is there a unilateral event mentioned in the next sentence of the story below?\\nThey then signed a different treaty in 1980.\\n### Response:\"\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda:0\").input_ids\n",
    "output = model.generate(input_ids=tokens, max_new_tokens=10, do_sample=True, temperature=0.8)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codebook, no quotes, no \"question\" works\n",
    "This right now is the only working one. The only bad thing about it is that it doesn't return single responses and it. doesn't listen to a preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the sentence describe an action or an interaction? Action describes an action, not an interaction, meaning that only one actor is taking an action. Interaction occurs in tandem with another actor and/or involve another actor.\n",
      "They then signed a different treaty in 1980.\n",
      "\n",
      "### Response:This sentence describes an interaction between two actors, the first one being the \"They\" who signed the treaty, and the second one being the other\n"
     ]
    }
   ],
   "source": [
    "coding_instructions = \"Does the sentence describe an action or an interaction? Action describes an action, not an interaction, meaning that only one actor is taking an action. Interaction occurs in tandem with another actor and/or involve another actor.\"\n",
    "sentence=\"They then signed a different treaty in 1980.\"\n",
    "prompt_template=coding_instructions + \"\\n\" + sentence + \"\\n\\n### Response:\"\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda:0\").input_ids\n",
    "output = model.generate(input_ids=tokens, max_new_tokens=30, do_sample=True, temperature=0.8)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the codebook instruction hurts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the sentence describe an action or an interaction?\n",
      "They then signed a different treaty in 1980.\n",
      "Action<|endoftext|>The sentence describes an action.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "coding_instructions = \"Does the sentence describe an action or an interaction?\"\n",
    "sentence=\"They then signed a different treaty in 1980.\"\n",
    "prompt_template=coding_instructions + \"\\n\" + sentence #+ \"\\n\\n### Response:\"\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda:0\").input_ids\n",
    "output = model.generate(input_ids=tokens, max_new_tokens=10, do_sample=True, temperature=0.8)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story Bolshevik Russia, precursor to the Soviet Union The Communist regime, which had attained power in Russia on 7 November 1917, opted to withdraw from World War I through a separate peace with Germany--the Treaty of Brest-Litovsk on 3 March 1918.\n",
      "Does the sentence describe an action or an interaction? Action describes an action, not an interaction, meaning that only one actor is taking an action. Interaction occurs in tandem with another actor and/or involve another actor.\n",
      "They then signed a different treaty in 1980.<|endoftext|>The sentence describes an interaction as it involves two\n"
     ]
    }
   ],
   "source": [
    "story= \"Story Bolshevik Russia, precursor to the Soviet Union The Communist regime, which had attained power in Russia on 7 November 1917, opted to withdraw from World War I through a separate peace with Germany--the Treaty of Brest-Litovsk on 3 March 1918.\"\n",
    "coding_instructions = \"Does the sentence describe an action or an interaction? Action describes an action, not an interaction, meaning that only one actor is taking an action. Interaction occurs in tandem with another actor and/or involve another actor.\"\n",
    "sentence=\"They then signed a different treaty in 1980.\"\n",
    "prompt_template = story + \"\\n\" + coding_instructions + \"\\n\" + sentence #+ \"\\n\\n### Response:\"\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").to(\"cuda:0\").input_ids\n",
    "output = model.generate(input_ids=tokens, max_new_tokens=10, do_sample=True, temperature=0.8)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
