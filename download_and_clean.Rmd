---
title: "corpus download and clean"
output: html_document
date: '2022-05-16'
---

```{r}
Sys.setenv(RETICULATE_PYTHON = "/home/skynet3/miniconda3/bin/python3")
library(reticulate)
use_python("/home/skynet3/miniconda3/bin/python3")
```

```{r}


df <- data.frame(crisno=1:496)
df$html <- NA
df$text <- NA
dim(df)
#http://www.icb.umd.edu/updates/v15/dataviewer/ajax/crisis_summary.asp?id=1&q=undefined
for(i in 1:496){
  url <- paste0("http://www.icb.umd.edu/updates/v15/dataviewer/ajax/crisis_summary.asp?id=",i,"&q=undefined")
  library(rvest)     
  page=read_html(url)
  text <- page %>% html_text2()
  df$text[i] <- text

}

library(tidyverse)
df %>% saveRDS(file="/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_in/crises_narratives_rex_2023_webscrape.Rds")
#library(arrow) ; #install.packages('arrow')
#df %>% write_tsv("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_in/crises_narratives_rex_2023_webscrape.tsv")

```



```{r}


library(tidyverse)
files <- list.files(path = "/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/crises_split/", 
                    pattern = NULL, all.files = FALSE,
                    full.names = T, recursive = FALSE,
                    ignore.case = FALSE, include.dirs = FALSE, no.. = FALSE)

file="/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/crises_split//crisis_225.csv"
df_list <- list()
for(file in files){
  print(file)
  #We should keep this because it's going to be clutch for debugging later
  df <- read_csv(file) %>% janitor::clean_names()
  df <- df %>% 
    mutate(file=basename(file)) %>%
    mutate(chunk_number=row_number()) %>% 
    
    mutate(sentences_raw = strsplit(as.character(sentences_raw), "\n")) %>%  #[1-9]\\. 
    unnest(sentences_raw) %>%
    mutate(sentences_clean= sentences_raw %>% str_replace("^[0-9]*\\.",'') %>% trimws()) %>%
    
    #Before we commit to numbering them, check for some bad cases
    #group_by(chunk_number) %>%
    #  mutate(chunk_sentence_number=row_number()) %>%
    #ungroup() %>%
    #arrange(chunks_class, chunk_sentence_number) %>%
    
    mutate(ends_in_comma = sentences_clean %>% str_detect(",$")) %>%
    mutate(lower_case = sentences_clean %>% str_detect("^[a-z]")) %>%
    mutate(lower_case_cumsum = cumsum(!lower_case) ) %>%        
    
    group_by(file, chunk_number, chunks, chunks_class, lower_case_cumsum) %>%
    summarise(sentences_clean = sentences_clean %>% paste0(collapse=" ")) %>% #This repairs sentences oversplit, next line starts with a lower case letter gets pulled up
    
    
    group_by(chunk_number) %>%
    mutate(chunk_sentence_number=row_number()) %>% #renumber now
    ungroup() %>%
    arrange(chunks_class, chunk_sentence_number) %>%
    
    
    mutate(sentences_final = ifelse(chunks_class %>% str_detect("1|2") & chunk_sentence_number==1, chunks, NA))  %>% 
    mutate(sentences_final = ifelse(chunks_class %>% str_detect("3"), sentences_clean, sentences_final) )  
  
  #df %>% filter(!is.na(sentences_final))
  df_list[[file]] <- df
  
}

df_allsentences_raw <- lapply(df_list,  function(x) x %>% mutate_all(as.character) ) %>% bind_rows()




#There are some errors we can repair, like one stence just ends in a comma and the next starts with a lower case letter
#Secret agreements among the Great Powers during World War I had awarded France a sphere of influence in Cilicia,
#that is, southeastern Anatolia bordering on Syria 

#  #Try classifying each chunk first
#def assemble_prompt(story):
#  prompt1 = """###: System: You are computer program that does exactly as instructed and nothing else.\n### User: ### Begin Text\n%s\n### End Text\n### Begin Question\nWhich of these answers best describes the above text? ### End Question\n### Begin Answer Choices\n1. A sentence fragment, like a section heading.\n2. One single complete sentence.\n3. More than one complete sentence.\n### Final Answer (a single number followed by a @ symbol)\n Assistant:""" % (story)
#return(prompt1)

#19,811
df_allsentences_clean <- df_allsentences_raw %>%
  filter(!is.na(sentences_final) & sentences_final!='') %>%
  mutate_at(c('chunk_number', 'chunk_sentence_number') , as.numeric) %>%
  
  arrange(file,chunk_number,chunk_sentence_number) %>%
  group_by(file) %>% 
  mutate(sentence_number=row_number()) %>% #renumber the sentences again
  ungroup() %>%
  mutate(crisno= (file %>% str_replace("crisis_",'')  %>% str_replace(".csv",'') %>% as.numeric) + 1 ) %>% #I apparently accidentally 0 indexes this
  select(crisno, chunk_number, chunks=chunks, chunks_class, chunk_sentence_number, sentence_number, sentences_final) %>%
  arrange(crisno,sentence_number) %>%
  mutate(chunks_class=chunks_class %>% str_extract("^[1-3]") %>% 
           case_match( "1" ~ "fragment", "2" ~ "sentence", "3" ~ "sentences" , .default = NA) )
dim(df_allsentences_clean)


df_allsentences_clean %>% 
  saveRDS("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_in/crises_narratives_rex_2023_webscrape_parsed_sentences.Rds")



crisis_narratives <-  read_tsv("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_in/crises_narratives_rex_2023_webscrape.tsv")
crisis_sentences <-  read_tsv("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_in/crises_narratives_rex_2023_webscrape_parsed_sentences.tsv")


```

## Now we create a crisis-sentence-event dataset

```{r}

crisis_narratives <-  read_tsv("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_in/crises_narratives_rex_2023_webscrape.tsv")
crisis_sentences <-  read_tsv("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_in/crises_narratives_rex_2023_webscrape_parsed_sentences.tsv")


library(tidyverse)
files <- list.files(path = "/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/events_split/", full.names = T)

file="/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/events_split//crisis_event_split_2.csv"
df_list <- list()
for(file in files){
  print(file)
  #We should keep this because it's going to be clutch for debugging later
  df <- read_csv(file) %>% janitor::clean_names()
  df <- df %>% 
    mutate(file=basename(file)) %>%
    mutate(event_split_output_final_answer = strsplit(as.character(event_split_output_final_answer), "\n")) %>%  #[1-9]\\. 
    unnest(event_split_output_final_answer) %>%
    
    mutate(event_split_output_final_answer= event_split_output_final_answer %>% str_replace("^[0-9]*\\.",'') %>% trimws()) %>%
    
    group_by(crisno, sentence_number) %>%
      mutate( sentence_event_number= row_number() ) %>%
    ungroup() %>%
    
    group_by(crisno) %>%
      mutate( event_number= row_number() ) %>% #event number now just means sentence or a sentence split into two subsentences
    ungroup() %>%

    mutate(event_text = ifelse(is.na(event_split_output_final_answer), sentences_final, event_split_output_final_answer) ) 
  
  #df %>% filter(!is.na(sentences_final))
  df_list[[file]] <- df
  
}

df_alleventtexts_raw <- lapply(df_list,  function(x) x %>% mutate_all(as.character) ) %>% bind_rows()

df_alleventtexts_raw %>% 
  saveRDS("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_in/crises_narratives_rex_2023_webscrape_parsed_sentences_parsed_events.Rds")



```




# Download the Original ICBe datasets
```{r}


# Download the ICBe datasets locally
source='https://raw.githubusercontent.com/CenterForPeaceAndSecurityStudies/ICBEdataset/master/replication_data/out/ICBe_V1.1_long_clean.Rds'
destination="./data_in/ICBe_V1.1_events_agreed_long.Rds"
if(!file.exists(destination)){
  download.file(url=source, destfile=destination, method="curl", quiet = FALSE)
}

ICBe_V11_events_agreed_long <- readRDS(destination)
ICBe_V11_events_agreed_long_196 <- ICBe_V11_events_agreed_long %>% filter(crisno==196)

source='https://raw.githubusercontent.com/CenterForPeaceAndSecurityStudies/ICBEdataset/master/replication_data/out/ICBe_V1.1_events_agreed.Rds'
destination="./data_in/ICBe_V1.1_events_agreed.Rds"
if(!file.exists(destination)){
  download.file(url=source, destfile=destination, method="curl", quiet = FALSE)
}

ICBe_V11_events_agreed <- readRDS(destination)

#we need a way to turn these into standardized JSON
library(tidyverse)
ICBe_V11_events_agreed_196 <- ICBe_V11_events_agreed %>% filter(crisno==196)

```


```{r setup, include=FALSE, eval=F}
knitr::opts_chunk$set(echo = TRUE)


library(jsonlite)
library(dplyr)
data <- jsonlite::read_json(links[1])


library(tidyverse)
library(rvest)
urls = paste0("http://www.icb.umd.edu/dataviewer/?crisno=",1:496)
loose_list <- list()
for(i in 1:length(urls)){
   try({
     df <- data.frame(text=urls[i] %>% read_html %>% html_text )  %>% 
       mutate(url=urls[i]) %>%
       mutate(crisno=i)
     loose_list[[file]] <- df
   })
}
 cuban_missile_loose <- bind_rows(loose_list) %>% 
   group_by(url) %>%
   summarise(text=paste(text, collapse="\n"))  %>% screen_rex()
 dim(cuban_missile_loose) #958
 cuban_missile_loose$short <- NA


library(stringr)
library(rvest)     
page=read_html("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_in/icb_narrative_links.html")
links <- page %>% html_nodes("a") %>% html_attr("href") %>% 
  stringr::str_replace(fixed("/view?usp=sharing"),"") %>% 
  stringr::str_replace(fixed("/view?usp=share_link"),"") %>% 
  stringr::str_replace(fixed("/edit?usp=sharing"),"") %>% paste0("&export=download")

temp <- tempfile(fileext = ".zip")
download.file(links[1],  temp)


library(curl)
curl_download(url="https://drive.google.com/uc?export=download&id=1Ap2HCkw3jgM0Sfp2SEMMV-WDl-ggpIoS",
              destfile="/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_in/crises/")
drive_download(file=)

download.file(link[1])


setwd("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_in/crises/")
library(googledrive)
for(link in links){
  drive_download(file=link)
}



fromscratch=F
if(fromscratch){
  urls <- readLines(here::here("replication_data", "in", "sheet1.xml.rels"))[[2]] %>%
    str_replace_all("Target=","\n") %>%
    str_replace_all("TargetMode=","\n") %>%  
    str_replace_all("Type=","\n") %>%
    str_split("\n") %>% unique() %>%
    as.data.frame() %>% setNames("V1") %>% 
    filter(!V1 %>% str_detect("hyperlink|External|Relationships")) %>%
    mutate(V1 = V1 %>% str_replace_all('\"','') %>% trimws() )
  urls$V1 %>% sapply(FUN= function(x) try({ drive_download(x) })  )
  #Google drive download
  library("googledrive")
  #https://drive.google.com/open?id=186JCTWqxQCCRBbLl0JDzeFEpJxijoyoB 
  setwd(here::here("replication_data", "in", "crisis_texts"))
  getwd()
  temp <- drive_download(file="https://drive.google.com/open?id=186JCTWqxQCCRBbLl0JDzeFEpJxijoyoB" ) # type = "csv",
}



#Nope the original csv files are broken. I'm going to have to reparse them from the website
#icb_long  %>%
#  dplyr::select(sentence_hash) %>% distinct() %>% View()
#There are 4 not in the icb long #397 403 451 459 474
setdiff(1:476, icb_long %>% dplyr::filter(varname=="crisno") %>% dplyr::pull(value) %>% unique() %>% as.numeric() %>% sort())
icb_crisis_summaries_20150604 <- read_csv(here::here("replication_data", "in", "icb_crisis_summaries_20150604.csv"))

icb_crisis_summaries_20150604_handadditions <- read_csv(here::here("replication_data", "in", "icb_crisis_summaries_20150604_handadditions.csv")) #this gets us 1 to 455

icb_crisis_summaries <- bind_rows(icb_crisis_summaries_20150604,icb_crisis_summaries_20150604_handadditions) %>%
  mutate(background = background %>% stringi::stri_enc_toutf8(validate=T)) %>% #there are non utf-8 characters
  mutate(text = text %>% stringi::stri_enc_toutf8(validate=T)) 




#459 is in ours but not the original
#397 403 451 459 there are 4 crises is in the text that aren't in our codings
setdiff(icb_crisis_summaries$crisno %>% unique(), icb_long %>% dplyr::filter(varname=="crisno") %>% dplyr::pull(value) %>% unique() %>% as.numeric() %>% sort()) #
#And none that our in our codings that aren't in the text
setdiff(icb_long %>% dplyr::filter(varname=="crisno") %>% dplyr::pull(value) %>% unique() %>% as.numeric() %>% sort(), icb_crisis_summaries$crisno %>% unique() ) #

```

```{r}


#Note I had to manually recreate the 001 crisis because it was just a dump of a large text file containing first 200ish crises

files <- list.files(path = here::here("replication_corpus", "data", "in", "crisis_texts/"), pattern = NULL, all.files = FALSE,
           full.names = FALSE, recursive = FALSE, ignore.case = FALSE, include.dirs = FALSE) 

filed_df <- data.frame(file=files) %>%
            mutate(filetype=NA) %>%
            mutate(filetype=ifelse(file %>% str_detect("pdf$"), "pdf", filetype)) %>%
            mutate(filetype=ifelse(file %>% str_detect("txt$"), "txt", filetype)) %>%
            mutate(filetype=ifelse(file %>% str_detect("docx$"), "docx", filetype)) %>%
            mutate(text=NA)
n=nrow(filed_df)
path <- here::here("replication_corpus", "data", "in", "crisis_texts/")
library(pdftools)
library(textreadr)
for(i in 1:n){
  if(filed_df$filetype[i]=="txt"){
    filed_df$text[i] <- path %>% paste0(filed_df$file[i]) %>% readLines() %>% paste0(collapse="\n") %>% str_replace_all("\n",' ')
  }
  if(filed_df$filetype[i]=="docx"){
    filed_df$text[i] <- path %>% paste0(filed_df$file[i])  %>% read_docx() %>% paste0(collapse="\n") %>% str_replace_all("\n",' ')
  }
  if(filed_df$filetype[i]=="pdf"){
    filed_df$text[i] <- path %>% paste0(filed_df$file[i])  %>% pdftools::pdf_text() %>% str_replace_all("1\n|2\n|3\n|4\n",' ') %>% paste0(collapse="\n") %>% str_replace_all("\n",' ')
  }
}

#Some had both a background and pre-crisis
filed_df_clean <- filed_df %>%
                  mutate(text = text %>%
                    str_replace_all("\\(see Background and Pre-crisis, Case #7\\)","(see Case #7\\)")  %>%  #replace one parenthetical that breaks things
                    str_replace_all("see Background to","")    %>%
                    str_replace_all("Summary of World Broadcasts","SWB")  %>%
                    str_replace_all(
  "BACKGROUND|PRE-CRISIS|Background and Pre-Crisis:|Background:|Crisis:|Sources:|References:|Summary:|Summary|Background and Pre-crisis|Background |Background |Pre-crisis |Sources |Summary |Precrisis: |Crisis: |Bibliography","\n")  %>% 
                    str_replace_all(" Crisis On the morning of","\nOn the morning of")  %>% 
                    str_replace_all("Crisis A crisis for Pakistan","\nA crisis for Pakistan") 
                  ) %>% 
                  mutate(text = strsplit(as.character(text), "\n")) %>% 
                  unnest(text) %>%
                  mutate(text_nchar=text %>% nchar() )  %>%
                  group_by(file) %>%
                    mutate(i=row_number()) %>%
                    mutate(crisis_title=text[1] %>% trimws() ) %>%
                    mutate(references=text[max(i)] %>% trimws() ) %>%
                    filter(i!=1) %>% #remove the title
                    filter(i!=max(i)) %>%  #remove the references
                  ungroup()  %>%
                  add_count(file) %>%
                  mutate(text=text %>% trimws() %>%
                    str_replace_all("^ {0,}: {0,}","") %>% 
                    trimws()
                    ) %>%
                  filter(text!='') %>%
                  mutate(crisno=file %>% str_replace_all("[^0-9]","") %>% substring(1,3) %>% as.numeric())
                  
filed_df_clean$file %>% table() %>% table()

icb_corpus <- filed_df_clean %>%
  arrange(crisno, i) %>%
  group_by(crisno) %>%
  summarise(
    crisis_title=crisis_title[1], 
    text=text %>% paste(collapse=" "),
    references=references[1],
    file=file[1]
  )

icb_corpus %>% saveRDS(here::here("replication_corpus", "data", "out", "icb_corpus_V1.0_May_16_2022.Rds"))

#icb_corpus$text %>% writeLines("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_corpus/data/in/test.txt")

```




